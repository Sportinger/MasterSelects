// React hook for WebGPU engine integration - Optimized

import { useEffect, useRef, useCallback } from 'react';
import { engine } from '../engine/WebGPUEngine';
import { useMixerStore } from '../stores/mixerStore';
import { useTimelineStore } from '../stores/timelineStore';
import { generateMaskTexture } from '../utils/maskRenderer';

export function useEngine() {
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const isEngineReady = useMixerStore((state) => state.isEngineReady);
  const isPlaying = useMixerStore((state) => state.isPlaying);
  const initRef = useRef(false);

  // Initialize engine - only once
  useEffect(() => {
    if (initRef.current) return;
    initRef.current = true;

    async function init() {
      const success = await engine.initialize();
      useMixerStore.getState().setEngineReady(success);
      if (success) {
        useMixerStore.getState().setPlaying(true);
      }
    }

    init();

    return () => {
      // Don't destroy on unmount - singleton should persist
    };
  }, []);

  // Set up canvas
  useEffect(() => {
    if (isEngineReady && canvasRef.current) {
      engine.setPreviewCanvas(canvasRef.current);
    }
  }, [isEngineReady]);

  // Track mask changes and update engine mask textures
  const maskVersionRef = useRef<Map<string, string>>(new Map());

  useEffect(() => {
    if (!isEngineReady) return;

    // Subscribe to both clips and playhead position changes
    const unsubscribe = useTimelineStore.subscribe(
      (state) => ({ clips: state.clips, playheadPosition: state.playheadPosition, tracks: state.tracks }),
      ({ clips, playheadPosition, tracks }) => {
        const layers = useMixerStore.getState().layers;

        // Get engine output dimensions (the actual render resolution)
        const engineDimensions = engine.getOutputDimensions();

        // Find clips at current playhead position
        const videoTracks = tracks.filter(t => t.type === 'video');
        const clipsAtTime = clips.filter(c =>
          playheadPosition >= c.startTime && playheadPosition < c.startTime + c.duration
        );

        // For each layer, find the corresponding clip
        for (let layerIndex = 0; layerIndex < layers.length; layerIndex++) {
          const layer = layers[layerIndex];
          const track = videoTracks[layerIndex];

          if (!track) continue;

          // Find clip for this track at current time
          const clip = clipsAtTime.find(c => c.trackId === track.id);

          if (clip?.masks && clip.masks.length > 0 && layer.source) {
            // Get source dimensions for aspect ratio calculation
            let sourceWidth = 1920, sourceHeight = 1080;
            if (layer.source.type === 'video' && layer.source.videoElement) {
              sourceWidth = layer.source.videoElement.videoWidth || 1920;
              sourceHeight = layer.source.videoElement.videoHeight || 1080;
            } else if (layer.source.type === 'image' && layer.source.imageElement) {
              sourceWidth = layer.source.imageElement.naturalWidth || 1920;
              sourceHeight = layer.source.imageElement.naturalHeight || 1080;
            }

            const sourceAspect = sourceWidth / sourceHeight;
            const outputAspect = engineDimensions.width / engineDimensions.height;
            const aspectRatio = sourceAspect / outputAspect;

            // Create a version string based on mask data, resolution AND aspect ratio
            const maskVersion = `${JSON.stringify(clip.masks)}_${engineDimensions.width}x${engineDimensions.height}_${aspectRatio.toFixed(4)}`;
            const cacheKey = `${clip.id}_${layer.id}`;
            const prevVersion = maskVersionRef.current.get(cacheKey);

            // Only regenerate if masks changed or resolution or aspect ratio changed
            if (maskVersion !== prevVersion) {
              maskVersionRef.current.set(cacheKey, maskVersion);

              // Transform mask vertices to account for aspect ratio fitting
              const transformedMasks = clip.masks.map(mask => ({
                ...mask,
                vertices: mask.vertices.map(v => {
                  let x = v.x, y = v.y;
                  // Apply inverse of video aspect ratio fitting
                  if (aspectRatio > 1.0) {
                    // Video is letterboxed - compress Y coordinates
                    y = (y - 0.5) / aspectRatio + 0.5;
                  } else {
                    // Video is pillarboxed - compress X coordinates
                    x = (x - 0.5) * aspectRatio + 0.5;
                  }
                  return { ...v, x, y };
                })
              }));

              // Generate mask texture at engine render resolution with transformed vertices
              const maskImageData = generateMaskTexture(
                transformedMasks,
                engineDimensions.width,
                engineDimensions.height
              );

              // Update engine with new mask texture
              engine.updateMaskTexture(layer.id, maskImageData);
            }
          } else {
            // No masks or no clip, clear the mask texture for this layer
            const cacheKey = `${clip?.id || 'none'}_${layer.id}`;
            if (maskVersionRef.current.has(cacheKey)) {
              maskVersionRef.current.delete(cacheKey);
              engine.removeMaskTexture(layer.id);
            }
          }
        }
      }
    );

    return () => unsubscribe();
  }, [isEngineReady]);

  // Render loop - optimized with layer snapshotting to prevent flickering
  useEffect(() => {
    if (!isEngineReady) return;

    // Stats update throttle
    let lastStatsUpdate = 0;

    const renderFrame = () => {
      try {
        // Always try to use cached frame first (works even during RAM preview rendering)
        const { playheadPosition, isRamPreviewing } = useTimelineStore.getState();
        if (engine.renderCachedFrame(playheadPosition)) {
          // Successfully rendered cached frame, skip live render
          return;
        }

        // CRITICAL: Skip live rendering during RAM Preview generation
        // Live rendering would seek videos and interfere with RAM Preview's seeking,
        // causing glitchy/wrong frames to be cached
        if (isRamPreviewing) {
          return;
        }

        // IMPORTANT: Snapshot layers array at start of frame to ensure consistency
        // This prevents flickering from reading partially updated state
        const layersSnapshot = useMixerStore.getState().layers;

        // Create a stable copy for this frame (shallow copy is sufficient
        // since we only read from it, never modify)
        const frameLayers = layersSnapshot.slice();

        // Render with snapshotted layers (engine handles empty layers by clearing to black)
        engine.render(frameLayers);

        // Throttle stats updates to reduce React re-renders (every 100ms for responsive display)
        const now = performance.now();
        if (now - lastStatsUpdate > 100) {
          useMixerStore.getState().setEngineStats(engine.getStats());
          lastStatsUpdate = now;
        }
      } catch (e) {
        console.error('Render error:', e);
      }
    };

    if (isPlaying) {
      engine.start(renderFrame);
    } else {
      engine.stop();
    }

    return () => {
      engine.stop();
    };
  }, [isEngineReady, isPlaying]);

  const createOutputWindow = useCallback((name: string) => {
    const id = `output_${Date.now()}`;
    return engine.createOutputWindow(id, name);
  }, []);

  const closeOutputWindow = useCallback((id: string) => {
    engine.closeOutputWindow(id);
  }, []);

  return {
    canvasRef,
    isEngineReady,
    isPlaying,
    createOutputWindow,
    closeOutputWindow,
  };
}
