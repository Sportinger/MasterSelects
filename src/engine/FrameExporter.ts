// Frame-by-frame exporter for precise video rendering
// Combines VideoEncoder and FrameExporter in one file to avoid import issues

import { Muxer as Mp4Muxer, ArrayBufferTarget as Mp4Target } from 'mp4-muxer';
import { Muxer as WebmMuxer, ArrayBufferTarget as WebmTarget } from 'webm-muxer';
import { engine } from './WebGPUEngine';
import { useTimelineStore } from '../stores/timeline';
import { useMediaStore } from '../stores/mediaStore';
import type { Layer, TimelineClip, NestedCompositionData } from '../types';
import { AudioExportPipeline, type AudioExportProgress, type EncodedAudioResult, AudioEncoderWrapper, type AudioCodec } from './audio';

// ============ TYPES ============

export interface ExportSettings {
  width: number;
  height: number;
  fps: number;
  codec: 'h264' | 'vp9';
  bitrate: number;
  startTime: number;
  endTime: number;
  // Audio settings
  includeAudio?: boolean;
  audioSampleRate?: 44100 | 48000;
  audioBitrate?: number;  // 128000 - 320000
  normalizeAudio?: boolean;
}

export interface ExportProgress {
  phase: 'video' | 'audio' | 'muxing';
  currentFrame: number;
  totalFrames: number;
  percent: number;
  estimatedTimeRemaining: number;
  currentTime: number;
  audioPhase?: AudioExportProgress['phase'];
  audioPercent?: number;
}

export interface FullExportSettings extends ExportSettings {
  filename?: string;
}

// ============ VIDEO ENCODER ============

type MuxerType = Mp4Muxer<Mp4Target> | WebmMuxer<WebmTarget>;

class VideoEncoderWrapper {
  private encoder: VideoEncoder | null = null;
  private muxer: MuxerType | null = null;
  private settings: ExportSettings;
  private encodedFrameCount = 0;
  private isClosed = false;
  private hasAudio = false;
  private audioCodec: AudioCodec = 'aac';
  private containerFormat: 'mp4' | 'webm' = 'mp4';

  constructor(settings: ExportSettings) {
    this.settings = settings;
    this.hasAudio = settings.includeAudio ?? false;
  }

  async init(): Promise<boolean> {
    if (!('VideoEncoder' in window)) {
      console.error('[VideoEncoder] WebCodecs not supported');
      return false;
    }

    // Detect audio codec if audio is enabled
    if (this.hasAudio) {
      const detectedCodec = await AudioEncoderWrapper.detectSupportedCodec();
      if (detectedCodec) {
        this.audioCodec = detectedCodec.codec;
        // Use WebM container for Opus, MP4 for AAC
        this.containerFormat = detectedCodec.codec === 'opus' ? 'webm' : 'mp4';
      } else {
        console.warn('[VideoEncoder] No audio codec supported, disabling audio');
        this.hasAudio = false;
      }
    }

    // Force VP9 for WebM container (H.264 not supported in WebM)
    const effectiveVideoCodec = this.containerFormat === 'webm' ? 'vp9' : this.settings.codec;
    const codecString = effectiveVideoCodec === 'h264'
      ? 'avc1.640028'
      : 'vp09.00.10.08';

    try {
      const support = await VideoEncoder.isConfigSupported({
        codec: codecString,
        width: this.settings.width,
        height: this.settings.height,
        bitrate: this.settings.bitrate,
        framerate: this.settings.fps,
      });

      if (!support.supported) {
        console.error('[VideoEncoder] Codec not supported:', codecString);
        return false;
      }
    } catch (e) {
      console.error('[VideoEncoder] Codec support check failed:', e);
      return false;
    }

    // Create appropriate muxer based on container format
    if (this.containerFormat === 'webm') {
      // WebM muxer for Opus audio
      if (this.hasAudio) {
        this.muxer = new WebmMuxer({
          target: new WebmTarget(),
          video: {
            codec: 'V_VP9',
            width: this.settings.width,
            height: this.settings.height,
          },
          audio: {
            codec: 'A_OPUS',
            sampleRate: this.settings.audioSampleRate ?? 48000,
            numberOfChannels: 2,
          },
        });
      } else {
        this.muxer = new WebmMuxer({
          target: new WebmTarget(),
          video: {
            codec: 'V_VP9',
            width: this.settings.width,
            height: this.settings.height,
          },
        });
      }
      console.log(`[VideoEncoder] Using WebM container with ${this.hasAudio ? 'Opus' : 'no'} audio`);
    } else {
      // MP4 muxer for AAC audio
      if (this.hasAudio) {
        this.muxer = new Mp4Muxer({
          target: new Mp4Target(),
          video: {
            codec: effectiveVideoCodec === 'h264' ? 'avc' : 'vp9',
            width: this.settings.width,
            height: this.settings.height,
          },
          audio: {
            codec: 'aac',
            sampleRate: this.settings.audioSampleRate ?? 48000,
            numberOfChannels: 2,
          },
          fastStart: 'in-memory',
        });
      } else {
        this.muxer = new Mp4Muxer({
          target: new Mp4Target(),
          video: {
            codec: effectiveVideoCodec === 'h264' ? 'avc' : 'vp9',
            width: this.settings.width,
            height: this.settings.height,
          },
          fastStart: 'in-memory',
        });
      }
      console.log(`[VideoEncoder] Using MP4 container with ${this.hasAudio ? 'AAC' : 'no'} audio`);
    }

    this.encoder = new VideoEncoder({
      output: (chunk, meta) => {
        if (this.muxer) {
          this.muxer.addVideoChunk(chunk, meta);
        }
        this.encodedFrameCount++;
      },
      error: (e) => {
        console.error('[VideoEncoder] Encode error:', e);
      },
    });

    await this.encoder.configure({
      codec: codecString,
      width: this.settings.width,
      height: this.settings.height,
      bitrate: this.settings.bitrate,
      framerate: this.settings.fps,
      latencyMode: 'quality',
      bitrateMode: 'variable',
    });

    console.log(`[VideoEncoder] Initialized: ${this.settings.width}x${this.settings.height} @ ${this.settings.fps}fps (${effectiveVideoCodec.toUpperCase()})`);
    return true;
  }

  getContainerFormat(): 'mp4' | 'webm' {
    return this.containerFormat;
  }

  getAudioCodec(): AudioCodec {
    return this.audioCodec;
  }

  async encodeFrame(pixels: Uint8ClampedArray, frameIndex: number): Promise<void> {
    if (!this.encoder || this.isClosed) {
      throw new Error('Encoder not initialized or already closed');
    }

    const timestampMicros = Math.round(frameIndex * (1_000_000 / this.settings.fps));
    const durationMicros = Math.round(1_000_000 / this.settings.fps);

    const frame = new VideoFrame(pixels.buffer, {
      format: 'RGBA',
      codedWidth: this.settings.width,
      codedHeight: this.settings.height,
      timestamp: timestampMicros,
      duration: durationMicros,
    });

    const keyFrame = frameIndex % 30 === 0;
    this.encoder.encode(frame, { keyFrame });
    frame.close();

    if (frameIndex % 10 === 0) {
      await new Promise(resolve => setTimeout(resolve, 0));
    }
  }

  /**
   * Add encoded audio chunks to the muxer
   */
  addAudioChunks(audioResult: EncodedAudioResult): void {
    if (!this.muxer || !this.hasAudio) {
      console.warn('[VideoEncoder] Cannot add audio: muxer not ready or audio not enabled');
      return;
    }

    console.log(`[VideoEncoder] Adding ${audioResult.chunks.length} audio chunks`);

    for (let i = 0; i < audioResult.chunks.length; i++) {
      const chunk = audioResult.chunks[i];
      const meta = audioResult.metadata[i];
      this.muxer.addAudioChunk(chunk, meta);
    }

    console.log(`[VideoEncoder] Audio chunks added successfully`);
  }

  async finish(): Promise<Blob> {
    if (!this.encoder || !this.muxer) {
      throw new Error('Encoder not initialized');
    }

    this.isClosed = true;
    await this.encoder.flush();
    this.encoder.close();
    this.muxer.finalize();

    const { buffer } = this.muxer.target;
    const mimeType = this.containerFormat === 'webm' ? 'video/webm' : 'video/mp4';

    console.log(`[VideoEncoder] Finished: ${this.encodedFrameCount} frames, ${(buffer.byteLength / 1024 / 1024).toFixed(2)}MB (${this.containerFormat.toUpperCase()})`);
    return new Blob([buffer], { type: mimeType });
  }

  cancel(): void {
    if (this.encoder && !this.isClosed) {
      this.isClosed = true;
      try {
        this.encoder.close();
      } catch {}
    }
  }
}

// ============ FRAME EXPORTER ============

export class FrameExporter {
  private settings: FullExportSettings;
  private encoder: VideoEncoderWrapper | null = null;
  private audioPipeline: AudioExportPipeline | null = null;
  private isCancelled = false;
  private frameTimes: number[] = [];

  constructor(settings: FullExportSettings) {
    this.settings = settings;
  }

  async export(onProgress: (progress: ExportProgress) => void): Promise<Blob | null> {
    const { fps, startTime, endTime, width, height, includeAudio } = this.settings;
    const frameDuration = 1 / fps;
    const totalFrames = Math.ceil((endTime - startTime) * fps);

    console.log(`[FrameExporter] Starting export: ${width}x${height} @ ${fps}fps, ${totalFrames} frames, audio: ${includeAudio ? 'yes' : 'no'}`);

    this.encoder = new VideoEncoderWrapper(this.settings);
    const initialized = await this.encoder.init();
    if (!initialized) {
      console.error('[FrameExporter] Failed to initialize encoder');
      return null;
    }

    // Initialize audio pipeline if audio is enabled
    if (includeAudio) {
      this.audioPipeline = new AudioExportPipeline({
        sampleRate: this.settings.audioSampleRate ?? 48000,
        bitrate: this.settings.audioBitrate ?? 256000,
        normalize: this.settings.normalizeAudio ?? false,
      });
    }

    const originalDimensions = engine.getOutputDimensions();
    engine.setResolution(width, height);

    try {
      // Phase 1: Encode video frames
      for (let frame = 0; frame < totalFrames; frame++) {
        if (this.isCancelled) {
          console.log('[FrameExporter] Export cancelled');
          this.encoder.cancel();
          this.audioPipeline?.cancel();
          engine.setResolution(originalDimensions.width, originalDimensions.height);
          return null;
        }

        const frameStart = performance.now();
        const time = startTime + frame * frameDuration;

        await this.seekAllClipsToTime(time);

        // Small delay to let video frames fully decode after seeking
        await new Promise(r => requestAnimationFrame(r));

        const layers = this.buildLayersAtTime(time);

        if (layers.length === 0) {
          console.warn('[FrameExporter] No layers at time', time);
        }

        engine.render(layers);

        const pixels = await engine.readPixels();
        if (!pixels) {
          console.error('[FrameExporter] Failed to read pixels at frame', frame);
          continue;
        }

        await this.encoder.encodeFrame(pixels, frame);

        const frameTime = performance.now() - frameStart;
        this.frameTimes.push(frameTime);
        if (this.frameTimes.length > 30) this.frameTimes.shift();

        const avgFrameTime = this.frameTimes.reduce((a, b) => a + b, 0) / this.frameTimes.length;
        const remainingFrames = totalFrames - frame - 1;
        // If audio is included, video is ~70% of total work
        const videoWeight = includeAudio ? 0.7 : 1.0;
        const videoPercent = ((frame + 1) / totalFrames) * 100 * videoWeight;
        const estimatedTimeRemaining = (remainingFrames * avgFrameTime) / 1000;

        onProgress({
          phase: 'video',
          currentFrame: frame + 1,
          totalFrames,
          percent: videoPercent,
          estimatedTimeRemaining,
          currentTime: time,
        });
      }

      // Phase 2: Export audio if enabled
      let audioResult: EncodedAudioResult | null = null;
      if (includeAudio && this.audioPipeline) {
        if (this.isCancelled) {
          engine.setResolution(originalDimensions.width, originalDimensions.height);
          return null;
        }

        console.log('[FrameExporter] Starting audio export...');

        audioResult = await this.audioPipeline.exportAudio(
          startTime,
          endTime,
          (audioProgress) => {
            if (this.isCancelled) return;

            // Audio is ~30% of total work (70-100%)
            const audioPercent = 70 + (audioProgress.percent * 0.3);

            onProgress({
              phase: 'audio',
              currentFrame: totalFrames,
              totalFrames,
              percent: audioPercent,
              estimatedTimeRemaining: 0, // Hard to estimate for audio
              currentTime: endTime,
              audioPhase: audioProgress.phase,
              audioPercent: audioProgress.percent,
            });
          }
        );

        // Add audio chunks to muxer if we have audio
        if (audioResult && audioResult.chunks.length > 0) {
          this.encoder.addAudioChunks(audioResult);
        } else {
          console.log('[FrameExporter] No audio to add (no audio clips in range or export failed)');
        }
      }

      const blob = await this.encoder.finish();
      console.log(`[FrameExporter] Export complete: ${(blob.size / 1024 / 1024).toFixed(2)}MB`);
      engine.setResolution(originalDimensions.width, originalDimensions.height);
      return blob;
    } catch (error) {
      console.error('[FrameExporter] Export error:', error);
      engine.setResolution(originalDimensions.width, originalDimensions.height);
      return null;
    }
  }

  cancel(): void {
    this.isCancelled = true;
    this.audioPipeline?.cancel();
  }

  private async seekAllClipsToTime(time: number): Promise<void> {
    const clips = useTimelineStore.getState().getClipsAtTime(time);
    const tracks = useTimelineStore.getState().tracks;
    const seekPromises: Promise<void>[] = [];

    // Debug: log clips found at this time (only first frame)
    if (time < 0.1) {
      console.log(`[FrameExporter] Clips at time ${time.toFixed(3)}:`, clips.map(c => ({
        name: c.name,
        isComposition: c.isComposition,
        hasVideoElement: !!c.source?.videoElement,
        sourceType: c.source?.type,
        trackId: c.trackId,
      })));
    }

    for (const clip of clips) {
      const track = tracks.find(t => t.id === clip.trackId);
      if (!track?.visible) continue;

      // Handle nested composition clips
      if (clip.isComposition && clip.nestedClips && clip.nestedTracks) {
        const clipLocalTime = time - clip.startTime;
        const nestedTime = clipLocalTime + (clip.inPoint || 0);

        for (const nestedClip of clip.nestedClips) {
          // Check if nested clip is active at this time
          if (nestedTime >= nestedClip.startTime && nestedTime < nestedClip.startTime + nestedClip.duration) {
            if (nestedClip.source?.videoElement) {
              const video = nestedClip.source.videoElement;
              const nestedLocalTime = nestedTime - nestedClip.startTime;
              const nestedClipTime = nestedClip.reversed
                ? nestedClip.outPoint - nestedLocalTime
                : nestedLocalTime + nestedClip.inPoint;
              seekPromises.push(this.seekVideo(video, nestedClipTime));
            }
          }
        }
        continue;
      }

      // Handle regular video clips
      if (clip.source?.type === 'video' && clip.source.videoElement) {
        const video = clip.source.videoElement;
        const clipLocalTime = time - clip.startTime;

        // Simple calculation matching nested clip approach
        // For clips with speed keyframes, use the store's calculation
        let clipTime: number;
        try {
          const sourceTime = useTimelineStore.getState().getSourceTimeForClip(clip.id, clipLocalTime);
          const initialSpeed = useTimelineStore.getState().getInterpolatedSpeed(clip.id, 0);
          const startPoint = initialSpeed >= 0 ? clip.inPoint : clip.outPoint;
          clipTime = Math.max(clip.inPoint, Math.min(clip.outPoint, startPoint + sourceTime));
        } catch (e) {
          // Fallback to simple calculation if speed functions fail
          clipTime = clip.reversed
            ? clip.outPoint - clipLocalTime
            : clipLocalTime + clip.inPoint;
          clipTime = Math.max(clip.inPoint, Math.min(clip.outPoint, clipTime));
        }

        console.log(`[FrameExporter] Seeking clip "${clip.name}" to ${clipTime.toFixed(3)}s (local: ${clipLocalTime.toFixed(3)}s)`);
        seekPromises.push(this.seekVideo(video, clipTime));
      }
    }

    await Promise.all(seekPromises);
  }

  private seekVideo(video: HTMLVideoElement, time: number): Promise<void> {
    return new Promise((resolve) => {
      const targetTime = Math.max(0, Math.min(time, video.duration || 0));

      // If already at correct time and ready, resolve immediately
      if (Math.abs(video.currentTime - targetTime) < 0.01 && !video.seeking && video.readyState >= 3) {
        resolve();
        return;
      }

      const timeout = setTimeout(() => {
        console.warn('[FrameExporter] Seek timeout at', targetTime);
        resolve();
      }, 2000); // Increased timeout

      const onSeeked = () => {
        clearTimeout(timeout);
        video.removeEventListener('seeked', onSeeked);

        // Wait for video to be fully ready with decoded frame data
        // readyState 3 = HAVE_FUTURE_DATA (enough to render current frame)
        // readyState 4 = HAVE_ENOUGH_DATA (can play through)
        let retries = 0;
        const maxRetries = 60; // ~1 second at 60fps

        const waitForReady = () => {
          retries++;
          if (!video.seeking && video.readyState >= 3) {
            // Give browser extra time to decode frame
            requestAnimationFrame(() => {
              requestAnimationFrame(() => resolve());
            });
          } else if (retries < maxRetries) {
            requestAnimationFrame(waitForReady);
          } else {
            console.warn('[FrameExporter] Video not ready after seek, proceeding anyway');
            resolve();
          }
        };
        waitForReady();
      };

      video.addEventListener('seeked', onSeeked);
      video.currentTime = targetTime;
    });
  }

  private buildLayersAtTime(time: number): Layer[] {
    const timelineState = useTimelineStore.getState();
    const { clips, tracks, getInterpolatedTransform, getInterpolatedEffects } = timelineState;
    const layers: Layer[] = [];

    const videoTracks = tracks.filter(t => t.type === 'video');
    const anyVideoSolo = videoTracks.some(t => t.solo);

    const isTrackVisible = (track: typeof videoTracks[0]) => {
      if (!track.visible) return false;
      if (anyVideoSolo) return track.solo;
      return true;
    };

    // Get clips at current time
    const clipsAtTime = clips.filter(
      c => time >= c.startTime && time < c.startTime + c.duration
    );

    // Build layers in track order (bottom to top)
    for (let trackIndex = 0; trackIndex < videoTracks.length; trackIndex++) {
      const track = videoTracks[trackIndex];
      if (!isTrackVisible(track)) continue;

      const clip = clipsAtTime.find(c => c.trackId === track.id);
      if (!clip) continue;

      const clipLocalTime = time - clip.startTime;

      // Get transform safely with defaults
      let transform;
      try {
        transform = getInterpolatedTransform(clip.id, clipLocalTime);
      } catch (e) {
        console.warn('[FrameExporter] Transform interpolation failed for clip', clip.id, e);
        transform = {
          position: { x: 0, y: 0, z: 0 },
          scale: { x: 1, y: 1 },
          rotation: { x: 0, y: 0, z: 0 },
          opacity: 1,
          blendMode: 'normal' as const,
        };
      }

      // Get effects safely
      let effects: typeof layers[0]['effects'] = [];
      try {
        effects = getInterpolatedEffects(clip.id, clipLocalTime);
      } catch (e) {
        console.warn('[FrameExporter] Effects interpolation failed for clip', clip.id, e);
      }

      const baseLayerProps = {
        id: `export_layer_${trackIndex}`,
        name: clip.name,
        visible: true,
        opacity: transform.opacity ?? 1,
        blendMode: transform.blendMode || 'normal',
        effects,
        position: {
          x: transform.position?.x ?? 0,
          y: transform.position?.y ?? 0,
          z: transform.position?.z ?? 0,
        },
        scale: {
          x: transform.scale?.x ?? 1,
          y: transform.scale?.y ?? 1,
        },
        rotation: {
          x: ((transform.rotation?.x ?? 0) * Math.PI) / 180,
          y: ((transform.rotation?.y ?? 0) * Math.PI) / 180,
          z: ((transform.rotation?.z ?? 0) * Math.PI) / 180,
        },
      };

      // Handle nested compositions
      if (clip.isComposition && clip.nestedClips && clip.nestedClips.length > 0) {
        const nestedLayers = this.buildNestedLayersForExport(clip, clipLocalTime + (clip.inPoint || 0));

        if (nestedLayers.length > 0) {
          const composition = useMediaStore.getState().compositions.find(c => c.id === clip.compositionId);
          const compWidth = composition?.width || 1920;
          const compHeight = composition?.height || 1080;

          const nestedCompData: NestedCompositionData = {
            compositionId: clip.compositionId || clip.id,
            layers: nestedLayers,
            width: compWidth,
            height: compHeight,
          };

          layers.push({
            ...baseLayerProps,
            source: {
              type: 'video',
              nestedComposition: nestedCompData,
            },
          });
        }
        continue;
      }

      // Handle video clips
      if (clip.source?.type === 'video' && clip.source.videoElement) {
        const video = clip.source.videoElement;
        // Only use video if it's ready
        if (video.readyState >= 2) {
          layers.push({
            ...baseLayerProps,
            source: {
              type: 'video',
              videoElement: video,
              webCodecsPlayer: clip.source.webCodecsPlayer,
            },
          });
        } else {
          console.warn('[FrameExporter] Video not ready for clip', clip.id, 'readyState:', video.readyState);
        }
      }
      // Handle image clips
      else if (clip.source?.type === 'image' && clip.source.imageElement) {
        layers.push({
          ...baseLayerProps,
          source: { type: 'image', imageElement: clip.source.imageElement },
        });
      }
      // Handle text clips
      else if (clip.source?.type === 'text' && clip.source.textCanvas) {
        layers.push({
          ...baseLayerProps,
          source: { type: 'text', textCanvas: clip.source.textCanvas },
        });
      }
    }

    return layers;
  }

  /**
   * Build layers for a nested composition at export time
   * Note: Video seeking is handled by seekAllClipsToTime, not here
   */
  private buildNestedLayersForExport(clip: TimelineClip, nestedTime: number): Layer[] {
    if (!clip.nestedClips || !clip.nestedTracks) return [];

    const nestedVideoTracks = clip.nestedTracks.filter(t => t.type === 'video' && t.visible);
    const layers: Layer[] = [];

    // Process tracks in same order as main timeline (0 to N, bottom to top)
    for (let i = 0; i < nestedVideoTracks.length; i++) {
      const nestedTrack = nestedVideoTracks[i];
      const nestedClip = clip.nestedClips.find(
        nc =>
          nc.trackId === nestedTrack.id &&
          nestedTime >= nc.startTime &&
          nestedTime < nc.startTime + nc.duration
      );

      if (!nestedClip) continue;

      const transform = nestedClip.transform || {
        position: { x: 0, y: 0, z: 0 },
        scale: { x: 1, y: 1 },
        rotation: { x: 0, y: 0, z: 0 },
        opacity: 1,
        blendMode: 'normal' as const,
      };

      const baseLayer = {
        id: `nested-export-${nestedClip.id}`,
        name: nestedClip.name,
        visible: true,
        opacity: transform.opacity ?? 1,
        blendMode: transform.blendMode || 'normal',
        effects: nestedClip.effects || [],
        position: {
          x: transform.position?.x || 0,
          y: transform.position?.y || 0,
          z: transform.position?.z || 0,
        },
        scale: {
          x: transform.scale?.x ?? 1,
          y: transform.scale?.y ?? 1,
        },
        rotation: {
          x: ((transform.rotation?.x || 0) * Math.PI) / 180,
          y: ((transform.rotation?.y || 0) * Math.PI) / 180,
          z: ((transform.rotation?.z || 0) * Math.PI) / 180,
        },
      };

      // Don't seek here - seekAllClipsToTime handles all seeking
      // Just build the layer with the video element
      if (nestedClip.source?.videoElement) {
        layers.push({
          ...baseLayer,
          source: {
            type: 'video',
            videoElement: nestedClip.source.videoElement,
            webCodecsPlayer: nestedClip.source.webCodecsPlayer,
          },
        } as Layer);
      } else if (nestedClip.source?.imageElement) {
        layers.push({
          ...baseLayer,
          source: {
            type: 'image',
            imageElement: nestedClip.source.imageElement,
          },
        } as Layer);
      } else if (nestedClip.source?.textCanvas) {
        layers.push({
          ...baseLayer,
          source: {
            type: 'text',
            textCanvas: nestedClip.source.textCanvas,
          },
        } as Layer);
      }
    }

    return layers;
  }

  static isSupported(): boolean {
    return 'VideoEncoder' in window && 'VideoFrame' in window;
  }

  static getPresetResolutions() {
    return [
      { label: '4K (3840x2160)', width: 3840, height: 2160 },
      { label: '1080p (1920x1080)', width: 1920, height: 1080 },
      { label: '720p (1280x720)', width: 1280, height: 720 },
      { label: '480p (854x480)', width: 854, height: 480 },
    ];
  }

  static getPresetFrameRates() {
    return [
      { label: '60 fps', fps: 60 },
      { label: '30 fps', fps: 30 },
      { label: '25 fps (PAL)', fps: 25 },
      { label: '24 fps (Film)', fps: 24 },
    ];
  }

  static getRecommendedBitrate(width: number, _height: number, _fps: number): number {
    if (width >= 3840) return 35_000_000;
    if (width >= 1920) return 15_000_000;
    if (width >= 1280) return 8_000_000;
    return 5_000_000;
  }
}

export function downloadBlob(blob: Blob, filename: string): void {
  const url = URL.createObjectURL(blob);
  const a = document.createElement('a');
  a.href = url;
  a.download = filename;
  document.body.appendChild(a);
  a.click();
  document.body.removeChild(a);
  URL.revokeObjectURL(url);
}
