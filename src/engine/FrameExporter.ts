// Frame-by-frame exporter for precise video rendering
// Combines VideoEncoder and FrameExporter in one file to avoid import issues

import { Muxer as Mp4Muxer, ArrayBufferTarget as Mp4Target } from 'mp4-muxer';
import { Muxer as WebmMuxer, ArrayBufferTarget as WebmTarget } from 'webm-muxer';
import { engine } from './WebGPUEngine';
import { useTimelineStore } from '../stores/timeline';
import { useMediaStore } from '../stores/mediaStore';
import type { Layer, TimelineClip, NestedCompositionData } from '../types';
import { AudioExportPipeline, type AudioExportProgress, type EncodedAudioResult, AudioEncoderWrapper, type AudioCodec } from './audio';

// ============ TYPES ============

export interface ExportSettings {
  width: number;
  height: number;
  fps: number;
  codec: 'h264' | 'vp9';
  bitrate: number;
  startTime: number;
  endTime: number;
  // Audio settings
  includeAudio?: boolean;
  audioSampleRate?: 44100 | 48000;
  audioBitrate?: number;  // 128000 - 320000
  normalizeAudio?: boolean;
}

export interface ExportProgress {
  phase: 'video' | 'audio' | 'muxing';
  currentFrame: number;
  totalFrames: number;
  percent: number;
  estimatedTimeRemaining: number;
  currentTime: number;
  audioPhase?: AudioExportProgress['phase'];
  audioPercent?: number;
}

export interface FullExportSettings extends ExportSettings {
  filename?: string;
}

// ============ VIDEO ENCODER ============

type MuxerType = Mp4Muxer<Mp4Target> | WebmMuxer<WebmTarget>;

class VideoEncoderWrapper {
  private encoder: VideoEncoder | null = null;
  private muxer: MuxerType | null = null;
  private settings: ExportSettings;
  private encodedFrameCount = 0;
  private isClosed = false;
  private hasAudio = false;
  private audioCodec: AudioCodec = 'aac';
  private containerFormat: 'mp4' | 'webm' = 'mp4';

  constructor(settings: ExportSettings) {
    this.settings = settings;
    this.hasAudio = settings.includeAudio ?? false;
  }

  async init(): Promise<boolean> {
    if (!('VideoEncoder' in window)) {
      console.error('[VideoEncoder] WebCodecs not supported');
      return false;
    }

    // Detect audio codec if audio is enabled
    if (this.hasAudio) {
      const detectedCodec = await AudioEncoderWrapper.detectSupportedCodec();
      if (detectedCodec) {
        this.audioCodec = detectedCodec.codec;
        // Use WebM container for Opus, MP4 for AAC
        this.containerFormat = detectedCodec.codec === 'opus' ? 'webm' : 'mp4';
      } else {
        console.warn('[VideoEncoder] No audio codec supported, disabling audio');
        this.hasAudio = false;
      }
    }

    // Force VP9 for WebM container (H.264 not supported in WebM)
    const effectiveVideoCodec = this.containerFormat === 'webm' ? 'vp9' : this.settings.codec;
    const codecString = effectiveVideoCodec === 'h264'
      ? 'avc1.640028'
      : 'vp09.00.10.08';

    try {
      const support = await VideoEncoder.isConfigSupported({
        codec: codecString,
        width: this.settings.width,
        height: this.settings.height,
        bitrate: this.settings.bitrate,
        framerate: this.settings.fps,
      });

      if (!support.supported) {
        console.error('[VideoEncoder] Codec not supported:', codecString);
        return false;
      }
    } catch (e) {
      console.error('[VideoEncoder] Codec support check failed:', e);
      return false;
    }

    // Create appropriate muxer based on container format
    if (this.containerFormat === 'webm') {
      // WebM muxer for Opus audio
      if (this.hasAudio) {
        this.muxer = new WebmMuxer({
          target: new WebmTarget(),
          video: {
            codec: 'V_VP9',
            width: this.settings.width,
            height: this.settings.height,
          },
          audio: {
            codec: 'A_OPUS',
            sampleRate: this.settings.audioSampleRate ?? 48000,
            numberOfChannels: 2,
          },
        });
      } else {
        this.muxer = new WebmMuxer({
          target: new WebmTarget(),
          video: {
            codec: 'V_VP9',
            width: this.settings.width,
            height: this.settings.height,
          },
        });
      }
      console.log(`[VideoEncoder] Using WebM container with ${this.hasAudio ? 'Opus' : 'no'} audio`);
    } else {
      // MP4 muxer for AAC audio
      if (this.hasAudio) {
        this.muxer = new Mp4Muxer({
          target: new Mp4Target(),
          video: {
            codec: effectiveVideoCodec === 'h264' ? 'avc' : 'vp9',
            width: this.settings.width,
            height: this.settings.height,
          },
          audio: {
            codec: 'aac',
            sampleRate: this.settings.audioSampleRate ?? 48000,
            numberOfChannels: 2,
          },
          fastStart: 'in-memory',
        });
      } else {
        this.muxer = new Mp4Muxer({
          target: new Mp4Target(),
          video: {
            codec: effectiveVideoCodec === 'h264' ? 'avc' : 'vp9',
            width: this.settings.width,
            height: this.settings.height,
          },
          fastStart: 'in-memory',
        });
      }
      console.log(`[VideoEncoder] Using MP4 container with ${this.hasAudio ? 'AAC' : 'no'} audio`);
    }

    this.encoder = new VideoEncoder({
      output: (chunk, meta) => {
        if (this.muxer) {
          this.muxer.addVideoChunk(chunk, meta);
        }
        this.encodedFrameCount++;
      },
      error: (e) => {
        console.error('[VideoEncoder] Encode error:', e);
      },
    });

    await this.encoder.configure({
      codec: codecString,
      width: this.settings.width,
      height: this.settings.height,
      bitrate: this.settings.bitrate,
      framerate: this.settings.fps,
      latencyMode: 'quality',
      bitrateMode: 'variable',
    });

    console.log(`[VideoEncoder] Initialized: ${this.settings.width}x${this.settings.height} @ ${this.settings.fps}fps (${effectiveVideoCodec.toUpperCase()})`);
    return true;
  }

  getContainerFormat(): 'mp4' | 'webm' {
    return this.containerFormat;
  }

  getAudioCodec(): AudioCodec {
    return this.audioCodec;
  }

  async encodeFrame(pixels: Uint8ClampedArray, frameIndex: number): Promise<void> {
    if (!this.encoder || this.isClosed) {
      throw new Error('Encoder not initialized or already closed');
    }

    const timestampMicros = Math.round(frameIndex * (1_000_000 / this.settings.fps));
    const durationMicros = Math.round(1_000_000 / this.settings.fps);

    const frame = new VideoFrame(pixels.buffer, {
      format: 'RGBA',
      codedWidth: this.settings.width,
      codedHeight: this.settings.height,
      timestamp: timestampMicros,
      duration: durationMicros,
    });

    const keyFrame = frameIndex % 30 === 0;
    this.encoder.encode(frame, { keyFrame });
    frame.close();

    if (frameIndex % 10 === 0) {
      await new Promise(resolve => setTimeout(resolve, 0));
    }
  }

  /**
   * Add encoded audio chunks to the muxer
   */
  addAudioChunks(audioResult: EncodedAudioResult): void {
    if (!this.muxer || !this.hasAudio) {
      console.warn('[VideoEncoder] Cannot add audio: muxer not ready or audio not enabled');
      return;
    }

    console.log(`[VideoEncoder] Adding ${audioResult.chunks.length} audio chunks`);

    for (let i = 0; i < audioResult.chunks.length; i++) {
      const chunk = audioResult.chunks[i];
      const meta = audioResult.metadata[i];
      this.muxer.addAudioChunk(chunk, meta);
    }

    console.log(`[VideoEncoder] Audio chunks added successfully`);
  }

  async finish(): Promise<Blob> {
    if (!this.encoder || !this.muxer) {
      throw new Error('Encoder not initialized');
    }

    this.isClosed = true;
    await this.encoder.flush();
    this.encoder.close();
    this.muxer.finalize();

    const { buffer } = this.muxer.target;
    const mimeType = this.containerFormat === 'webm' ? 'video/webm' : 'video/mp4';

    console.log(`[VideoEncoder] Finished: ${this.encodedFrameCount} frames, ${(buffer.byteLength / 1024 / 1024).toFixed(2)}MB (${this.containerFormat.toUpperCase()})`);
    return new Blob([buffer], { type: mimeType });
  }

  cancel(): void {
    if (this.encoder && !this.isClosed) {
      this.isClosed = true;
      try {
        this.encoder.close();
      } catch {}
    }
  }
}

// ============ FRAME EXPORTER ============

export class FrameExporter {
  private settings: FullExportSettings;
  private encoder: VideoEncoderWrapper | null = null;
  private audioPipeline: AudioExportPipeline | null = null;
  private isCancelled = false;
  private frameTimes: number[] = [];

  constructor(settings: FullExportSettings) {
    this.settings = settings;
  }

  async export(onProgress: (progress: ExportProgress) => void): Promise<Blob | null> {
    const { fps, startTime, endTime, width, height, includeAudio } = this.settings;
    const frameDuration = 1 / fps;
    const totalFrames = Math.ceil((endTime - startTime) * fps);

    console.log(`[FrameExporter] Starting export: ${width}x${height} @ ${fps}fps, ${totalFrames} frames, audio: ${includeAudio ? 'yes' : 'no'}`);

    this.encoder = new VideoEncoderWrapper(this.settings);
    const initialized = await this.encoder.init();
    if (!initialized) {
      console.error('[FrameExporter] Failed to initialize encoder');
      return null;
    }

    // Initialize audio pipeline if audio is enabled
    if (includeAudio) {
      this.audioPipeline = new AudioExportPipeline({
        sampleRate: this.settings.audioSampleRate ?? 48000,
        bitrate: this.settings.audioBitrate ?? 256000,
        normalize: this.settings.normalizeAudio ?? false,
      });
    }

    const originalDimensions = engine.getOutputDimensions();
    engine.setResolution(width, height);

    try {
      // Phase 1: Encode video frames
      for (let frame = 0; frame < totalFrames; frame++) {
        if (this.isCancelled) {
          console.log('[FrameExporter] Export cancelled');
          this.encoder.cancel();
          this.audioPipeline?.cancel();
          engine.setResolution(originalDimensions.width, originalDimensions.height);
          return null;
        }

        const frameStart = performance.now();
        const time = startTime + frame * frameDuration;

        await this.seekAllClipsToTime(time);
        const layers = this.buildLayersAtTime(time);

        engine.render(layers);

        const pixels = await engine.readPixels();
        if (!pixels) {
          console.error('[FrameExporter] Failed to read pixels at frame', frame);
          continue;
        }

        await this.encoder.encodeFrame(pixels, frame);

        const frameTime = performance.now() - frameStart;
        this.frameTimes.push(frameTime);
        if (this.frameTimes.length > 30) this.frameTimes.shift();

        const avgFrameTime = this.frameTimes.reduce((a, b) => a + b, 0) / this.frameTimes.length;
        const remainingFrames = totalFrames - frame - 1;
        // If audio is included, video is ~70% of total work
        const videoWeight = includeAudio ? 0.7 : 1.0;
        const videoPercent = ((frame + 1) / totalFrames) * 100 * videoWeight;
        const estimatedTimeRemaining = (remainingFrames * avgFrameTime) / 1000;

        onProgress({
          phase: 'video',
          currentFrame: frame + 1,
          totalFrames,
          percent: videoPercent,
          estimatedTimeRemaining,
          currentTime: time,
        });
      }

      // Phase 2: Export audio if enabled
      let audioResult: EncodedAudioResult | null = null;
      if (includeAudio && this.audioPipeline) {
        if (this.isCancelled) {
          engine.setResolution(originalDimensions.width, originalDimensions.height);
          return null;
        }

        console.log('[FrameExporter] Starting audio export...');

        audioResult = await this.audioPipeline.exportAudio(
          startTime,
          endTime,
          (audioProgress) => {
            if (this.isCancelled) return;

            // Audio is ~30% of total work (70-100%)
            const audioPercent = 70 + (audioProgress.percent * 0.3);

            onProgress({
              phase: 'audio',
              currentFrame: totalFrames,
              totalFrames,
              percent: audioPercent,
              estimatedTimeRemaining: 0, // Hard to estimate for audio
              currentTime: endTime,
              audioPhase: audioProgress.phase,
              audioPercent: audioProgress.percent,
            });
          }
        );

        // Add audio chunks to muxer if we have audio
        if (audioResult && audioResult.chunks.length > 0) {
          this.encoder.addAudioChunks(audioResult);
        } else {
          console.log('[FrameExporter] No audio to add (no audio clips in range or export failed)');
        }
      }

      const blob = await this.encoder.finish();
      console.log(`[FrameExporter] Export complete: ${(blob.size / 1024 / 1024).toFixed(2)}MB`);
      engine.setResolution(originalDimensions.width, originalDimensions.height);
      return blob;
    } catch (error) {
      console.error('[FrameExporter] Export error:', error);
      engine.setResolution(originalDimensions.width, originalDimensions.height);
      return null;
    }
  }

  cancel(): void {
    this.isCancelled = true;
    this.audioPipeline?.cancel();
  }

  private async seekAllClipsToTime(time: number): Promise<void> {
    const clips = useTimelineStore.getState().getClipsAtTime(time);
    const tracks = useTimelineStore.getState().tracks;
    const seekPromises: Promise<void>[] = [];

    for (const clip of clips) {
      const track = tracks.find(t => t.id === clip.trackId);
      if (!track?.visible) continue;

      // Handle nested composition clips
      if (clip.isComposition && clip.nestedClips && clip.nestedTracks) {
        const clipLocalTime = time - clip.startTime;
        const nestedTime = clipLocalTime + (clip.inPoint || 0);

        for (const nestedClip of clip.nestedClips) {
          // Check if nested clip is active at this time
          if (nestedTime >= nestedClip.startTime && nestedTime < nestedClip.startTime + nestedClip.duration) {
            if (nestedClip.source?.videoElement) {
              const video = nestedClip.source.videoElement;
              const nestedLocalTime = nestedTime - nestedClip.startTime;
              const nestedClipTime = nestedClip.reversed
                ? nestedClip.outPoint - nestedLocalTime
                : nestedLocalTime + nestedClip.inPoint;
              seekPromises.push(this.seekVideo(video, nestedClipTime));
            }
          }
        }
        continue;
      }

      // Handle regular video clips
      if (clip.source?.type === 'video' && clip.source.videoElement) {
        const video = clip.source.videoElement;
        const clipLocalTime = time - clip.startTime;
        // Calculate source time using speed integration (handles keyframes)
        const sourceTime = useTimelineStore.getState().getSourceTimeForClip(clip.id, clipLocalTime);
        // Determine start point based on INITIAL speed (speed at t=0), not clip.speed
        // This is important when keyframes change speed throughout the clip
        const initialSpeed = useTimelineStore.getState().getInterpolatedSpeed(clip.id, 0);
        const startPoint = initialSpeed >= 0 ? clip.inPoint : clip.outPoint;
        const clipTime = Math.max(clip.inPoint, Math.min(clip.outPoint, startPoint + sourceTime));
        seekPromises.push(this.seekVideo(video, clipTime));
      }
    }

    await Promise.all(seekPromises);
  }

  private seekVideo(video: HTMLVideoElement, time: number): Promise<void> {
    return new Promise((resolve) => {
      if (Math.abs(video.currentTime - time) < 0.001 && !video.seeking && video.readyState >= 2) {
        resolve();
        return;
      }

      const timeout = setTimeout(() => {
        console.warn('[FrameExporter] Seek timeout at', time);
        resolve();
      }, 1000);

      const onSeeked = () => {
        clearTimeout(timeout);
        video.removeEventListener('seeked', onSeeked);

        // Wait for video to be fully ready (not seeking, has data)
        const waitForReady = () => {
          if (!video.seeking && video.readyState >= 2) {
            // Give browser one more frame to finalize
            requestAnimationFrame(() => resolve());
          } else {
            requestAnimationFrame(waitForReady);
          }
        };
        waitForReady();
      };

      video.addEventListener('seeked', onSeeked);
      video.currentTime = Math.max(0, Math.min(time, video.duration || 0));
    });
  }

  private buildLayersAtTime(time: number): Layer[] {
    const timelineState = useTimelineStore.getState();
    const { clips, tracks, getInterpolatedTransform, getInterpolatedEffects } = timelineState;
    const layers: Layer[] = [];

    const videoTracks = tracks.filter(t => t.type === 'video');
    const anyVideoSolo = videoTracks.some(t => t.solo);

    const isTrackVisible = (track: typeof videoTracks[0]) => {
      if (!track.visible) return false;
      if (anyVideoSolo) return track.solo;
      return true;
    };

    // Get clips at current time
    const clipsAtTime = clips.filter(
      c => time >= c.startTime && time < c.startTime + c.duration
    );

    // Build layers in track order (bottom to top)
    for (let trackIndex = 0; trackIndex < videoTracks.length; trackIndex++) {
      const track = videoTracks[trackIndex];
      if (!isTrackVisible(track)) continue;

      const clip = clipsAtTime.find(c => c.trackId === track.id);
      if (!clip) continue;

      const clipLocalTime = time - clip.startTime;
      const transform = getInterpolatedTransform(clip.id, clipLocalTime);
      const effects = getInterpolatedEffects(clip.id, clipLocalTime);

      // Handle nested compositions
      if (clip.isComposition && clip.nestedClips && clip.nestedClips.length > 0) {
        const nestedLayers = this.buildNestedLayersForExport(clip, clipLocalTime + (clip.inPoint || 0));

        if (nestedLayers.length > 0) {
          const composition = useMediaStore.getState().compositions.find(c => c.id === clip.compositionId);
          const compWidth = composition?.width || 1920;
          const compHeight = composition?.height || 1080;

          const nestedCompData: NestedCompositionData = {
            compositionId: clip.compositionId || clip.id,
            layers: nestedLayers,
            width: compWidth,
            height: compHeight,
          };

          layers.push({
            id: `export_layer_${trackIndex}`,
            name: clip.name,
            visible: true,
            opacity: transform.opacity,
            blendMode: transform.blendMode,
            source: {
              type: 'video',
              nestedComposition: nestedCompData,
            },
            effects,
            position: { x: transform.position.x, y: transform.position.y, z: transform.position.z },
            scale: { x: transform.scale.x, y: transform.scale.y },
            rotation: {
              x: transform.rotation.x * (Math.PI / 180),
              y: transform.rotation.y * (Math.PI / 180),
              z: transform.rotation.z * (Math.PI / 180),
            },
          });
        }
        continue;
      }

      // Handle video clips
      if (clip.source?.type === 'video' && clip.source.videoElement) {
        layers.push({
          id: `export_layer_${trackIndex}`,
          name: clip.name,
          visible: true,
          opacity: transform.opacity,
          blendMode: transform.blendMode,
          source: {
            type: 'video',
            videoElement: clip.source.videoElement,
            webCodecsPlayer: clip.source.webCodecsPlayer,
          },
          effects,
          position: { x: transform.position.x, y: transform.position.y, z: transform.position.z },
          scale: { x: transform.scale.x, y: transform.scale.y },
          rotation: {
            x: transform.rotation.x * (Math.PI / 180),
            y: transform.rotation.y * (Math.PI / 180),
            z: transform.rotation.z * (Math.PI / 180),
          },
        });
      }
      // Handle image clips
      else if (clip.source?.type === 'image' && clip.source.imageElement) {
        layers.push({
          id: `export_layer_${trackIndex}`,
          name: clip.name,
          visible: true,
          opacity: transform.opacity,
          blendMode: transform.blendMode,
          source: { type: 'image', imageElement: clip.source.imageElement },
          effects,
          position: { x: transform.position.x, y: transform.position.y, z: transform.position.z },
          scale: { x: transform.scale.x, y: transform.scale.y },
          rotation: {
            x: transform.rotation.x * (Math.PI / 180),
            y: transform.rotation.y * (Math.PI / 180),
            z: transform.rotation.z * (Math.PI / 180),
          },
        });
      }
      // Handle text clips
      else if (clip.source?.type === 'text' && clip.source.textCanvas) {
        layers.push({
          id: `export_layer_${trackIndex}`,
          name: clip.name,
          visible: true,
          opacity: transform.opacity,
          blendMode: transform.blendMode,
          source: { type: 'text', textCanvas: clip.source.textCanvas },
          effects,
          position: { x: transform.position.x, y: transform.position.y, z: transform.position.z },
          scale: { x: transform.scale.x, y: transform.scale.y },
          rotation: {
            x: transform.rotation.x * (Math.PI / 180),
            y: transform.rotation.y * (Math.PI / 180),
            z: transform.rotation.z * (Math.PI / 180),
          },
        });
      }
    }

    return layers;
  }

  /**
   * Build layers for a nested composition at export time
   */
  private buildNestedLayersForExport(clip: TimelineClip, nestedTime: number): Layer[] {
    if (!clip.nestedClips || !clip.nestedTracks) return [];

    const nestedVideoTracks = clip.nestedTracks.filter(t => t.type === 'video' && t.visible);
    const layers: Layer[] = [];

    for (let i = nestedVideoTracks.length - 1; i >= 0; i--) {
      const nestedTrack = nestedVideoTracks[i];
      const nestedClip = clip.nestedClips.find(
        nc =>
          nc.trackId === nestedTrack.id &&
          nestedTime >= nc.startTime &&
          nestedTime < nc.startTime + nc.duration
      );

      if (!nestedClip) continue;

      const nestedLocalTime = nestedTime - nestedClip.startTime;
      const nestedClipTime = nestedClip.reversed
        ? nestedClip.outPoint - nestedLocalTime
        : nestedLocalTime + nestedClip.inPoint;

      const transform = nestedClip.transform || {
        position: { x: 0, y: 0, z: 0 },
        scale: { x: 1, y: 1 },
        rotation: { x: 0, y: 0, z: 0 },
        opacity: 1,
        blendMode: 'normal' as const,
      };

      const baseLayer = {
        id: `nested-export-${nestedClip.id}`,
        name: nestedClip.name,
        visible: true,
        opacity: transform.opacity ?? 1,
        blendMode: transform.blendMode || 'normal',
        effects: nestedClip.effects || [],
        position: {
          x: transform.position?.x || 0,
          y: transform.position?.y || 0,
          z: transform.position?.z || 0,
        },
        scale: {
          x: transform.scale?.x ?? 1,
          y: transform.scale?.y ?? 1,
        },
        rotation: {
          x: ((transform.rotation?.x || 0) * Math.PI) / 180,
          y: ((transform.rotation?.y || 0) * Math.PI) / 180,
          z: ((transform.rotation?.z || 0) * Math.PI) / 180,
        },
      };

      // Seek nested video to correct time
      if (nestedClip.source?.videoElement) {
        const video = nestedClip.source.videoElement;
        if (Math.abs(video.currentTime - nestedClipTime) > 0.01) {
          video.currentTime = nestedClipTime;
        }

        layers.push({
          ...baseLayer,
          source: {
            type: 'video',
            videoElement: video,
            webCodecsPlayer: nestedClip.source.webCodecsPlayer,
          },
        } as Layer);
      } else if (nestedClip.source?.imageElement) {
        layers.push({
          ...baseLayer,
          source: {
            type: 'image',
            imageElement: nestedClip.source.imageElement,
          },
        } as Layer);
      } else if (nestedClip.source?.textCanvas) {
        layers.push({
          ...baseLayer,
          source: {
            type: 'text',
            textCanvas: nestedClip.source.textCanvas,
          },
        } as Layer);
      }
    }

    return layers;
  }

  static isSupported(): boolean {
    return 'VideoEncoder' in window && 'VideoFrame' in window;
  }

  static getPresetResolutions() {
    return [
      { label: '4K (3840x2160)', width: 3840, height: 2160 },
      { label: '1080p (1920x1080)', width: 1920, height: 1080 },
      { label: '720p (1280x720)', width: 1280, height: 720 },
      { label: '480p (854x480)', width: 854, height: 480 },
    ];
  }

  static getPresetFrameRates() {
    return [
      { label: '60 fps', fps: 60 },
      { label: '30 fps', fps: 30 },
      { label: '25 fps (PAL)', fps: 25 },
      { label: '24 fps (Film)', fps: 24 },
    ];
  }

  static getRecommendedBitrate(width: number, _height: number, _fps: number): number {
    if (width >= 3840) return 35_000_000;
    if (width >= 1920) return 15_000_000;
    if (width >= 1280) return 8_000_000;
    return 5_000_000;
  }
}

export function downloadBlob(blob: Blob, filename: string): void {
  const url = URL.createObjectURL(blob);
  const a = document.createElement('a');
  a.href = url;
  a.download = filename;
  document.body.appendChild(a);
  a.click();
  document.body.removeChild(a);
  URL.revokeObjectURL(url);
}
