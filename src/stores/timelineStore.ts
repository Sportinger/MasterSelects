// Timeline store for video editing functionality

import { create } from 'zustand';
import { subscribeWithSelector } from 'zustand/middleware';
import type { TimelineClip, TimelineTrack, ClipTransform, CompositionTimelineData, SerializableClip, Keyframe, AnimatableProperty, EasingType, ClipMask, MaskVertex, Effect, EffectType } from '../types';
import { useMediaStore } from './mediaStore';
import { useMixerStore } from './mixerStore';
import { getInterpolatedClipTransform, getKeyframeAtTime, hasKeyframesForProperty, interpolateKeyframes } from '../utils/keyframeInterpolation';

// Default transform for new clips
const DEFAULT_TRANSFORM: ClipTransform = {
  opacity: 1,
  blendMode: 'normal',
  position: { x: 0, y: 0, z: 0 },
  scale: { x: 1, y: 1 },
  rotation: { x: 0, y: 0, z: 0 },
};

// Generate waveform data from audio file
async function generateWaveform(file: File, sampleCount: number = 200): Promise<number[]> {
  try {
    const audioContext = new AudioContext();
    const arrayBuffer = await file.arrayBuffer();
    const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

    const channelData = audioBuffer.getChannelData(0); // Use first channel
    const samples: number[] = [];
    const blockSize = Math.floor(channelData.length / sampleCount);

    for (let i = 0; i < sampleCount; i++) {
      const start = i * blockSize;
      const end = start + blockSize;
      let sum = 0;

      for (let j = start; j < end; j++) {
        sum += Math.abs(channelData[j]);
      }

      samples.push(sum / blockSize);
    }

    // Normalize to 0-1 range
    const max = Math.max(...samples);
    if (max > 0) {
      await audioContext.close();
      return samples.map(s => s / max);
    }
    await audioContext.close();
    return samples;
  } catch (e) {
    console.warn('Failed to generate waveform:', e);
    return [];
  }
}

// Generate thumbnail filmstrip from video
async function generateThumbnails(video: HTMLVideoElement, duration: number, count: number = 10): Promise<string[]> {
  const thumbnails: string[] = [];
  const canvas = document.createElement('canvas');
  const ctx = canvas.getContext('2d');
  if (!ctx) return thumbnails;

  // Thumbnail dimensions (aspect ratio preserved)
  const thumbHeight = 40;
  const thumbWidth = Math.round((video.videoWidth / video.videoHeight) * thumbHeight);
  canvas.width = thumbWidth;
  canvas.height = thumbHeight;

  // Generate frames at regular intervals
  const interval = duration / count;

  for (let i = 0; i < count; i++) {
    const time = i * interval;
    try {
      await seekVideo(video, time);
      ctx.drawImage(video, 0, 0, thumbWidth, thumbHeight);
      thumbnails.push(canvas.toDataURL('image/jpeg', 0.6));
    } catch (e) {
      console.warn('Failed to generate thumbnail at', time, e);
    }
  }

  return thumbnails;
}

// Helper to seek video and wait for it to be ready
function seekVideo(video: HTMLVideoElement, time: number): Promise<void> {
  return new Promise((resolve, reject) => {
    const timeout = setTimeout(() => reject(new Error('Seek timeout')), 3000);

    const onSeeked = () => {
      clearTimeout(timeout);
      video.removeEventListener('seeked', onSeeked);
      resolve();
    };

    video.addEventListener('seeked', onSeeked);
    video.currentTime = time;
  });
}

// Snap threshold in seconds (clips will snap when within this distance)
const SNAP_THRESHOLD_SECONDS = 0.1;

interface TimelineStore {
  // State
  tracks: TimelineTrack[];
  clips: TimelineClip[];
  playheadPosition: number;
  duration: number;
  zoom: number;  // pixels per second
  scrollX: number;
  isPlaying: boolean;
  isDraggingPlayhead: boolean;  // Track when user is scrubbing
  selectedClipId: string | null;

  // In/Out markers for work area
  inPoint: number | null;
  outPoint: number | null;

  // Loop playback between in/out points
  loopPlayback: boolean;
  setLoopPlayback: (loop: boolean) => void;
  toggleLoopPlayback: () => void;

  // RAM Preview - pre-render frames to memory for instant scrubbing
  ramPreviewEnabled: boolean;  // Master toggle for RAM Preview feature
  ramPreviewProgress: number | null;  // null = not previewing, 0-100 = progress
  ramPreviewRange: { start: number; end: number } | null;  // cached time range
  isRamPreviewing: boolean;
  toggleRamPreviewEnabled: () => void;
  startRamPreview: () => Promise<void>;
  cancelRamPreview: () => void;
  clearRamPreview: () => void;

  // Playback frame caching (like After Effects' green line)
  cachedFrameTimes: Set<number>;  // Set of quantized times that are cached
  addCachedFrame: (time: number) => void;
  getCachedRanges: () => Array<{ start: number; end: number }>;
  invalidateCache: () => void;  // Clear cache when content changes

  // Track actions
  addTrack: (type: 'video' | 'audio') => void;
  removeTrack: (id: string) => void;
  setTrackMuted: (id: string, muted: boolean) => void;
  setTrackVisible: (id: string, visible: boolean) => void;
  setTrackSolo: (id: string, solo: boolean) => void;
  setTrackHeight: (id: string, height: number) => void;
  scaleTracksOfType: (type: 'video' | 'audio', delta: number) => void;



  // Clip actions
  addClip: (trackId: string, file: File, startTime: number, estimatedDuration?: number, mediaFileId?: string) => Promise<void>;
  addCompClip: (trackId: string, composition: import('./mediaStore').Composition, startTime: number) => void;
  removeClip: (id: string) => void;
  moveClip: (id: string, newStartTime: number, newTrackId?: string, skipLinked?: boolean) => void;
  trimClip: (id: string, inPoint: number, outPoint: number) => void;
  splitClip: (clipId: string, splitTime: number) => void;
  splitClipAtPlayhead: () => void;
  selectClip: (id: string | null) => void;
  updateClipTransform: (id: string, transform: Partial<ClipTransform>) => void;
  toggleClipReverse: (id: string) => void;

  // Clip effect actions
  addClipEffect: (clipId: string, effectType: string) => void;
  removeClipEffect: (clipId: string, effectId: string) => void;
  updateClipEffect: (clipId: string, effectId: string, params: Partial<Effect['params']>) => void;

  // Playback actions
  setPlayheadPosition: (position: number) => void;
  setDraggingPlayhead: (dragging: boolean) => void;
  play: () => void;
  pause: () => void;
  stop: () => void;

  // View actions
  setZoom: (zoom: number) => void;
  setScrollX: (scrollX: number) => void;

  // In/Out marker actions
  setInPoint: (time: number | null) => void;
  setOutPoint: (time: number | null) => void;
  clearInOut: () => void;
  setInPointAtPlayhead: () => void;
  setOutPointAtPlayhead: () => void;

  // Utils
  getClipsAtTime: (time: number) => TimelineClip[];
  updateDuration: () => void;
  findAvailableAudioTrack: (startTime: number, duration: number) => string;
  getSnappedPosition: (clipId: string, desiredStartTime: number, trackId: string) => { startTime: number; snapped: boolean };
  findNonOverlappingPosition: (clipId: string, desiredStartTime: number, trackId: string, duration: number) => number;

  // Composition timeline save/load
  getSerializableState: () => CompositionTimelineData;
  loadState: (data: CompositionTimelineData | undefined) => Promise<void>;
  clearTimeline: () => void;

  // Keyframe animation state
  clipKeyframes: Map<string, Keyframe[]>;     // clipId -> keyframes
  keyframeRecordingEnabled: Set<string>;      // "clipId:property" keys for recording mode
  expandedTracks: Set<string>;                // Tracks with expanded property rows
  expandedTrackPropertyGroups: Map<string, Set<string>>; // trackId -> expanded group names
  selectedKeyframeIds: Set<string>;           // Currently selected keyframe IDs

  // Keyframe actions
  addKeyframe: (clipId: string, property: AnimatableProperty, value: number, time?: number, easing?: EasingType) => void;
  removeKeyframe: (keyframeId: string) => void;
  updateKeyframe: (keyframeId: string, updates: Partial<Omit<Keyframe, 'id' | 'clipId'>>) => void;
  moveKeyframe: (keyframeId: string, newTime: number) => void;
  getClipKeyframes: (clipId: string) => Keyframe[];
  getInterpolatedTransform: (clipId: string, clipLocalTime: number) => ClipTransform;
  getInterpolatedEffects: (clipId: string, clipLocalTime: number) => Effect[];
  hasKeyframes: (clipId: string, property?: AnimatableProperty) => boolean;

  // Keyframe recording mode
  toggleKeyframeRecording: (clipId: string, property: AnimatableProperty) => void;
  isRecording: (clipId: string, property: AnimatableProperty) => boolean;
  setPropertyValue: (clipId: string, property: AnimatableProperty, value: number) => void;

  // Keyframe UI state
  toggleTrackExpanded: (trackId: string) => void;
  isTrackExpanded: (trackId: string) => boolean;
  toggleTrackPropertyGroupExpanded: (trackId: string, groupName: string) => void;
  isTrackPropertyGroupExpanded: (trackId: string, groupName: string) => boolean;
  getExpandedTrackHeight: (trackId: string, baseHeight: number) => number;
  selectKeyframe: (keyframeId: string, addToSelection?: boolean) => void;
  deselectAllKeyframes: () => void;
  deleteSelectedKeyframes: () => void;
  trackHasKeyframes: (trackId: string) => boolean;

  // Mask management
  maskEditMode: 'none' | 'drawing' | 'editing' | 'drawingRect' | 'drawingEllipse' | 'drawingPen';
  activeMaskId: string | null;
  selectedVertexIds: Set<string>;
  maskDrawStart: { x: number; y: number } | null;  // Start point for drag-to-draw
  setMaskEditMode: (mode: 'none' | 'drawing' | 'editing' | 'drawingRect' | 'drawingEllipse' | 'drawingPen') => void;
  setMaskDrawStart: (point: { x: number; y: number } | null) => void;
  setActiveMask: (clipId: string | null, maskId: string | null) => void;
  selectVertex: (vertexId: string, addToSelection?: boolean) => void;
  deselectAllVertices: () => void;

  // Mask CRUD
  addMask: (clipId: string, mask?: Partial<ClipMask>) => string;
  removeMask: (clipId: string, maskId: string) => void;
  updateMask: (clipId: string, maskId: string, updates: Partial<ClipMask>) => void;
  reorderMasks: (clipId: string, fromIndex: number, toIndex: number) => void;
  getClipMasks: (clipId: string) => ClipMask[];

  // Vertex CRUD
  addVertex: (clipId: string, maskId: string, vertex: Omit<MaskVertex, 'id'>, index?: number) => string;
  removeVertex: (clipId: string, maskId: string, vertexId: string) => void;
  updateVertex: (clipId: string, maskId: string, vertexId: string, updates: Partial<MaskVertex>) => void;
  closeMask: (clipId: string, maskId: string) => void;

  // Preset shapes
  addRectangleMask: (clipId: string) => string;
  addEllipseMask: (clipId: string) => string;
}

const DEFAULT_TRACKS: TimelineTrack[] = [
  { id: 'video-1', name: 'Video 1', type: 'video', height: 60, muted: false, visible: true, solo: false },
  { id: 'video-2', name: 'Video 2', type: 'video', height: 60, muted: false, visible: true, solo: false },
  { id: 'audio-1', name: 'Audio', type: 'audio', height: 40, muted: false, visible: true, solo: false },
];

export const useTimelineStore = create<TimelineStore>()(
  subscribeWithSelector((set, get) => ({
    tracks: DEFAULT_TRACKS,
    clips: [],
    playheadPosition: 0,
    duration: 60, // Default 60 seconds
    zoom: 50, // 50 pixels per second
    scrollX: 0,
    isPlaying: false,
    isDraggingPlayhead: false,
    selectedClipId: null,
    inPoint: null,
    outPoint: null,
    loopPlayback: false,
    ramPreviewEnabled: false,  // RAM Preview off by default
    ramPreviewProgress: null,
    ramPreviewRange: null,
    isRamPreviewing: false,
    cachedFrameTimes: new Set<number>(),

    // Keyframe animation state
    clipKeyframes: new Map<string, Keyframe[]>(),
    keyframeRecordingEnabled: new Set<string>(),
    expandedTracks: new Set<string>(),
    expandedTrackPropertyGroups: new Map<string, Set<string>>(),
    selectedKeyframeIds: new Set<string>(),

    // Mask state
    maskEditMode: 'none' as 'none' | 'drawing' | 'editing' | 'drawingRect' | 'drawingEllipse' | 'drawingPen',
    activeMaskId: null as string | null,
    selectedVertexIds: new Set<string>(),
    maskDrawStart: null as { x: number; y: number } | null,

    // Track actions
    addTrack: (type) => {
      const { tracks } = get();
      const typeCount = tracks.filter(t => t.type === type).length + 1;
      const newTrack: TimelineTrack = {
        id: `${type}-${Date.now()}`,
        name: `${type === 'video' ? 'Video' : 'Audio'} ${typeCount}`,
        type,
        height: type === 'video' ? 60 : 40,
        muted: false,
        visible: true,
        solo: false,
      };

      // Video tracks: insert at TOP (before all existing video tracks)
      // Audio tracks: insert at BOTTOM (after all existing audio tracks)
      if (type === 'video') {
        // Insert at index 0 (top of timeline)
        set({ tracks: [newTrack, ...tracks] });
      } else {
        // Audio: append at end (bottom of timeline)
        set({ tracks: [...tracks, newTrack] });
      }
    },

    removeTrack: (id) => {
      const { tracks, clips } = get();
      set({
        tracks: tracks.filter(t => t.id !== id),
        clips: clips.filter(c => c.trackId !== id),
      });
    },

    setTrackMuted: (id, muted) => {
      const { tracks } = get();
      set({
        tracks: tracks.map(t => t.id === id ? { ...t, muted } : t),
      });
      // Audio changes don't affect video cache
    },

    setTrackVisible: (id, visible) => {
      const { tracks } = get();
      const track = tracks.find(t => t.id === id);
      set({
        tracks: tracks.map(t => t.id === id ? { ...t, visible } : t),
      });
      // Invalidate cache if video track visibility changed
      if (track?.type === 'video') {
        get().invalidateCache();
      }
    },

    setTrackSolo: (id, solo) => {
      const { tracks } = get();
      const track = tracks.find(t => t.id === id);
      set({
        tracks: tracks.map(t => t.id === id ? { ...t, solo } : t),
      });
      // Invalidate cache if video track solo changed
      if (track?.type === 'video') {
        get().invalidateCache();
      }
    },

    setTrackHeight: (id, height) => {
      const { tracks } = get();
      set({
        tracks: tracks.map(t => t.id === id ? { ...t, height: Math.max(30, Math.min(200, height)) } : t),
      });
    },

    scaleTracksOfType: (type, delta) => {
      const { tracks } = get();
      const tracksOfType = tracks.filter(t => t.type === type);

      if (tracksOfType.length === 0) return;

      // Find the max height among tracks of this type
      const maxHeight = Math.max(...tracksOfType.map(t => t.height));

      // First call: sync all to max height (if they differ)
      // Subsequent calls: scale uniformly
      const allSameHeight = tracksOfType.every(t => t.height === maxHeight);

      if (!allSameHeight && delta !== 0) {
        // Sync all to max height first
        set({
          tracks: tracks.map(t =>
            t.type === type ? { ...t, height: maxHeight } : t
          ),
        });
      } else {
        // All already synced, scale uniformly
        const newHeight = Math.max(30, Math.min(200, maxHeight + delta));
        set({
          tracks: tracks.map(t =>
            t.type === type ? { ...t, height: newHeight } : t
          ),
        });
      }
    },

    // Clip actions
    addClip: async (trackId, file, startTime, providedDuration, mediaFileId) => {
      const isVideo = file.type.startsWith('video/');
      const isAudio = file.type.startsWith('audio/');
      const isImage = file.type.startsWith('image/');

      // Validate track type matches media type
      const { tracks } = get();
      const targetTrack = tracks.find(t => t.id === trackId);
      if (!targetTrack) {
        console.warn('[Timeline] Track not found:', trackId);
        return;
      }

      // Video/image files can only go on video tracks
      if ((isVideo || isImage) && targetTrack.type !== 'video') {
        console.warn('[Timeline] Cannot add video/image to audio track');
        return;
      }

      // Audio files can only go on audio tracks
      if (isAudio && targetTrack.type !== 'audio') {
        console.warn('[Timeline] Cannot add audio to video track');
        return;
      }

      const clipId = `clip-${Date.now()}`;
      const audioClipId = isVideo ? `clip-audio-${Date.now()}` : undefined;

      // Use provided duration or estimate (will be updated when media loads)
      const estimatedDuration = providedDuration ?? 5;

      // Helper to update clip when loaded
      const updateClip = (id: string, updates: Partial<TimelineClip>) => {
        const currentClips = get().clips;
        set({
          clips: currentClips.map(c => c.id === id ? { ...c, ...updates } : c)
        });
        get().updateDuration();
      };

      // For video: add loading placeholder for both video and audio clips immediately
      if (isVideo) {
        const { findAvailableAudioTrack, clips: currentClips, updateDuration } = get();
        const audioTrackId = findAvailableAudioTrack(startTime, estimatedDuration);

        // Create loading placeholder clips immediately
        const videoClip: TimelineClip = {
          id: clipId,
          trackId,
          name: file.name,
          file,
          startTime,
          duration: estimatedDuration,
          inPoint: 0,
          outPoint: estimatedDuration,
          source: { type: 'video', naturalDuration: estimatedDuration, mediaFileId },
          linkedClipId: audioTrackId ? audioClipId : undefined,
          transform: { ...DEFAULT_TRANSFORM },
          effects: [],
          isLoading: true,
        };

        const clipsToAdd: TimelineClip[] = [videoClip];

        if (audioTrackId && audioClipId) {
          const audioClip: TimelineClip = {
            id: audioClipId,
            trackId: audioTrackId,
            name: `${file.name} (Audio)`,
            file,
            startTime,
            duration: estimatedDuration,
            inPoint: 0,
            outPoint: estimatedDuration,
            source: { type: 'audio', naturalDuration: estimatedDuration },
            linkedClipId: clipId,
            transform: { ...DEFAULT_TRANSFORM },
          effects: [],
            isLoading: true,
          };
          clipsToAdd.push(audioClip);
        }

        set({ clips: [...currentClips, ...clipsToAdd] });
        updateDuration();

        // Now load media in background
        // Always create HTMLVideoElement for thumbnails and fallback
        const video = document.createElement('video');
        video.src = URL.createObjectURL(file);
        video.preload = 'auto';
        video.muted = true;
        video.crossOrigin = 'anonymous';

        // Wait for metadata
        await new Promise<void>((resolve) => {
          video.onloadedmetadata = () => resolve();
          video.onerror = () => resolve();
        });

        const naturalDuration = video.duration || estimatedDuration;

        // Update clip duration immediately once we know it
        updateClip(clipId, {
          duration: naturalDuration,
          outPoint: naturalDuration,
          source: { type: 'video', videoElement: video, naturalDuration, mediaFileId },
        });
        if (audioTrackId && audioClipId) {
          updateClip(audioClipId, {
            duration: naturalDuration,
            outPoint: naturalDuration,
          });
        }

        // Mark clip as ready immediately - thumbnails will load in background
        updateClip(clipId, {
          source: {
            type: 'video',
            videoElement: video,
            naturalDuration,
            mediaFileId,
          },
          isLoading: false,
        });

        // Generate thumbnails in background (non-blocking)
        (async () => {
          try {
            // Wait for video to be ready for thumbnails
            await new Promise<void>((resolve) => {
              if (video.readyState >= 2) {
                resolve();
              } else {
                video.oncanplay = () => resolve();
                setTimeout(resolve, 2000); // Timeout fallback
              }
            });

            const thumbnails = await generateThumbnails(video, naturalDuration);
            console.log(`[Timeline] Generated ${thumbnails.length} thumbnails for ${file.name}`);

            // Update clip with thumbnails
            const currentClips = get().clips;
            set({
              clips: currentClips.map(c => c.id === clipId ? { ...c, thumbnails } : c)
            });

            // Seek back to start
            video.currentTime = 0;
          } catch (e) {
            console.warn('Failed to generate thumbnails:', e);
          }
        })();

        // Load audio - make it ready immediately, waveform loads in background
        if (audioTrackId && audioClipId) {
          const audioFromVideo = document.createElement('audio');
          audioFromVideo.src = URL.createObjectURL(file);
          audioFromVideo.preload = 'auto';

          // Mark audio clip as ready immediately
          updateClip(audioClipId, {
            source: { type: 'audio', audioElement: audioFromVideo, naturalDuration },
            isLoading: false,
          });

          // Generate waveform in background (non-blocking)
          (async () => {
            try {
              const audioWaveform = await generateWaveform(file);
              const currentClips = get().clips;
              set({
                clips: currentClips.map(c => c.id === audioClipId ? { ...c, waveform: audioWaveform } : c)
              });
            } catch (e) {
              console.warn('Failed to generate waveform:', e);
            }
          })();
        }

        // Sync to media store
        const mediaStore = useMediaStore.getState();
        if (!mediaStore.getFileByName(file.name)) {
          mediaStore.importFile(file);
        }
        // Invalidate RAM preview cache - new video content added
        get().invalidateCache();

        return;
      }

      // For audio: add loading placeholder immediately
      if (isAudio) {
        const { clips: currentClips, updateDuration } = get();

        const audioClip: TimelineClip = {
          id: clipId,
          trackId,
          name: file.name,
          file,
          startTime,
          duration: estimatedDuration,
          inPoint: 0,
          outPoint: estimatedDuration,
          source: { type: 'audio', naturalDuration: estimatedDuration },
          transform: { ...DEFAULT_TRANSFORM },
          effects: [],
          isLoading: true,
        };

        set({ clips: [...currentClips, audioClip] });
        updateDuration();

        // Load audio in background
        const audio = document.createElement('audio');
        audio.src = URL.createObjectURL(file);
        audio.preload = 'metadata';

        await new Promise<void>((resolve) => {
          audio.onloadedmetadata = () => resolve();
          audio.onerror = () => resolve();
        });

        const naturalDuration = audio.duration || estimatedDuration;

        // Generate waveform
        let waveform: number[] = [];
        try {
          waveform = await generateWaveform(file);
        } catch (e) {
          console.warn('Failed to generate waveform:', e);
        }

        updateClip(clipId, {
          duration: naturalDuration,
          outPoint: naturalDuration,
          source: { type: 'audio', audioElement: audio, naturalDuration },
          waveform,
          isLoading: false,
        });

        // Sync to media store
        const mediaStore = useMediaStore.getState();
        if (!mediaStore.getFileByName(file.name)) {
          mediaStore.importFile(file);
        }
        // Invalidate RAM preview cache - audio affects composition
        get().invalidateCache();

        return;
      }

      // For images: add loading placeholder immediately
      if (isImage) {
        const { clips: currentClips, updateDuration } = get();

        const imageClip: TimelineClip = {
          id: clipId,
          trackId,
          name: file.name,
          file,
          startTime,
          duration: estimatedDuration,
          inPoint: 0,
          outPoint: estimatedDuration,
          source: { type: 'image', naturalDuration: estimatedDuration },
          transform: { ...DEFAULT_TRANSFORM },
          effects: [],
          isLoading: true,
        };

        set({ clips: [...currentClips, imageClip] });
        updateDuration();

        // Load image in background
        const img = new Image();
        img.src = URL.createObjectURL(file);

        await new Promise<void>((resolve) => {
          img.onload = () => resolve();
          img.onerror = () => resolve();
        });

        // Generate thumbnail
        let thumbnails: string[] = [];
        const canvas = document.createElement('canvas');
        const ctx = canvas.getContext('2d');
        if (ctx) {
          const thumbHeight = 40;
          const thumbWidth = Math.round((img.width / img.height) * thumbHeight);
          canvas.width = thumbWidth;
          canvas.height = thumbHeight;
          ctx.drawImage(img, 0, 0, thumbWidth, thumbHeight);
          thumbnails = [canvas.toDataURL('image/jpeg', 0.6)];
        }

        updateClip(clipId, {
          source: { type: 'image', imageElement: img, naturalDuration: estimatedDuration },
          thumbnails,
          isLoading: false,
        });

        // Sync to media store
        const mediaStore = useMediaStore.getState();
        if (!mediaStore.getFileByName(file.name)) {
          mediaStore.importFile(file);
        }
        // Invalidate RAM preview cache - new content added
        get().invalidateCache();
      }
    },

    // Add a composition as a clip (nested composition)
    addCompClip: async (trackId, composition, startTime) => {
      const { clips, updateDuration, findNonOverlappingPosition } = get();

      const clipId = `clip-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;

      // Find non-overlapping position
      const finalStartTime = findNonOverlappingPosition(clipId, startTime, trackId, composition.duration);

      // Create placeholder clip immediately (will be updated with nested content)
      const compClip: TimelineClip = {
        id: clipId,
        trackId,
        name: composition.name,
        file: new File([], composition.name), // Placeholder file
        startTime: finalStartTime,
        duration: composition.duration,
        inPoint: 0,
        outPoint: composition.duration,
        source: {
          type: 'video', // Comp clips are treated as video
          naturalDuration: composition.duration,
        },
        transform: { ...DEFAULT_TRANSFORM },
          effects: [],
        isLoading: true,
        isComposition: true,
        compositionId: composition.id,
        nestedClips: [],
        nestedTracks: [],
      };

      set({ clips: [...clips, compClip] });
      updateDuration();

      // Load nested composition's clips in background
      if (composition.timelineData) {
        const mediaStore = useMediaStore.getState();
        const nestedClips: TimelineClip[] = [];
        const nestedTracks = composition.timelineData.tracks;

        for (const serializedClip of composition.timelineData.clips) {
          // Find the media file
          const mediaFile = mediaStore.files.find(f => f.id === serializedClip.mediaFileId);
          if (!mediaFile || !mediaFile.file) {
            console.warn('[Nested Comp] Could not find media file for clip:', serializedClip.name);
            continue;
          }

          // Create the clip with loading state
          const nestedClip: TimelineClip = {
            id: `nested-${clipId}-${serializedClip.id}`,
            trackId: serializedClip.trackId,
            name: serializedClip.name,
            file: mediaFile.file,
            startTime: serializedClip.startTime,
            duration: serializedClip.duration,
            inPoint: serializedClip.inPoint,
            outPoint: serializedClip.outPoint,
            source: null,
            thumbnails: serializedClip.thumbnails,
            linkedClipId: serializedClip.linkedClipId,
            waveform: serializedClip.waveform,
            transform: serializedClip.transform,
            effects: serializedClip.effects || [],
            isLoading: true,
          };

          nestedClips.push(nestedClip);

          // Load media element async
          const type = serializedClip.sourceType;
          const fileUrl = URL.createObjectURL(mediaFile.file);

          if (type === 'video') {
            const video = document.createElement('video');
            video.src = fileUrl;
            video.muted = true;
            video.playsInline = true;
            video.preload = 'auto';
            video.crossOrigin = 'anonymous';

            video.addEventListener('canplaythrough', () => {
              nestedClip.source = {
                type: 'video',
                videoElement: video,
                naturalDuration: video.duration,
              };
              nestedClip.isLoading = false;
              // Trigger re-render by updating the parent clip
              const currentClips = get().clips;
              set({ clips: [...currentClips] });
            }, { once: true });
          } else if (type === 'audio') {
            const audio = document.createElement('audio');
            audio.src = fileUrl;
            audio.preload = 'auto';

            audio.addEventListener('canplaythrough', () => {
              nestedClip.source = {
                type: 'audio',
                audioElement: audio,
                naturalDuration: audio.duration,
              };
              nestedClip.isLoading = false;
            }, { once: true });
          } else if (type === 'image') {
            const img = new Image();
            img.src = fileUrl;

            img.addEventListener('load', () => {
              nestedClip.source = {
                type: 'image',
                imageElement: img,
              };
              nestedClip.isLoading = false;
              const currentClips = get().clips;
              set({ clips: [...currentClips] });
            }, { once: true });
          }
        }

        // Update the comp clip with nested data
        const currentClips = get().clips;
        set({
          clips: currentClips.map(c =>
            c.id === clipId
              ? { ...c, nestedClips, nestedTracks, isLoading: false }
              : c
          ),
        });

        // Generate thumbnails from first video in nested comp
        const firstVideoClip = nestedClips.find(c => c.file.type.startsWith('video/'));
        if (firstVideoClip) {
          // Wait a bit for video to load
          setTimeout(async () => {
            const video = firstVideoClip.source?.videoElement;
            if (video && video.readyState >= 2) {
              try {
                const thumbnails = await generateThumbnails(video, composition.duration);
                const updatedClips = get().clips;
                set({
                  clips: updatedClips.map(c =>
                    c.id === clipId ? { ...c, thumbnails } : c
                  ),
                });
              } catch (e) {
                console.warn('[Nested Comp] Failed to generate thumbnails:', e);
              }
            }
          }, 500);
        }
      } else {
        // No timeline data - just mark as loaded
        const currentClips = get().clips;
        set({
          clips: currentClips.map(c =>
            c.id === clipId ? { ...c, isLoading: false } : c
          ),
        });
      }

      get().invalidateCache();
    },

    removeClip: (id) => {
      const { clips, selectedClipId, updateDuration } = get();
      set({
        clips: clips.filter(c => c.id !== id),
        selectedClipId: selectedClipId === id ? null : selectedClipId,
      });
      updateDuration();
      // Invalidate RAM preview cache - content changed
      get().invalidateCache();
    },

    moveClip: (id, newStartTime, newTrackId, skipLinked = false) => {
      const { clips, tracks, updateDuration, getSnappedPosition, findNonOverlappingPosition } = get();
      const movingClip = clips.find(c => c.id === id);
      if (!movingClip) return;

      const targetTrackId = newTrackId ?? movingClip.trackId;

      // Validate track type if changing tracks
      if (newTrackId && newTrackId !== movingClip.trackId) {
        const targetTrack = tracks.find(t => t.id === newTrackId);
        const sourceType = movingClip.source?.type;

        if (targetTrack && sourceType) {
          // Video/image clips can only go on video tracks
          if ((sourceType === 'video' || sourceType === 'image') && targetTrack.type !== 'video') {
            console.warn('[Timeline] Cannot move video/image to audio track');
            return;
          }
          // Audio clips can only go on audio tracks
          if (sourceType === 'audio' && targetTrack.type !== 'audio') {
            console.warn('[Timeline] Cannot move audio to video track');
            return;
          }
        }
      }

      // Apply snapping first
      const { startTime: snappedTime } = getSnappedPosition(id, newStartTime, targetTrackId);

      // Then find non-overlapping position
      const finalStartTime = findNonOverlappingPosition(id, snappedTime, targetTrackId, movingClip.duration);

      // Calculate time delta to apply to linked clips
      const timeDelta = finalStartTime - movingClip.startTime;

      // For linked clip, also find non-overlapping position
      const linkedClip = clips.find(c => c.id === movingClip.linkedClipId || c.linkedClipId === id);
      let linkedFinalTime = linkedClip ? linkedClip.startTime + timeDelta : 0;
      if (linkedClip && !skipLinked) {
        linkedFinalTime = findNonOverlappingPosition(
          linkedClip.id,
          linkedClip.startTime + timeDelta,
          linkedClip.trackId,
          linkedClip.duration
        );
      }

      set({
        clips: clips.map(c => {
          // Move the primary clip
          if (c.id === id) {
            return {
              ...c,
              startTime: Math.max(0, finalStartTime),
              trackId: targetTrackId,
            };
          }
          // Also move linked clip (keep it in sync) - unless skipLinked is true
          if (!skipLinked && (c.id === movingClip.linkedClipId || c.linkedClipId === id)) {
            return {
              ...c,
              startTime: Math.max(0, linkedFinalTime),
              // Keep linked clip on its own track (don't change track)
            };
          }
          return c;
        }),
      });
      updateDuration();
      // Invalidate RAM preview cache - content changed
      get().invalidateCache();
    },

    trimClip: (id, inPoint, outPoint) => {
      const { clips, updateDuration } = get();
      set({
        clips: clips.map(c => {
          if (c.id !== id) return c;
          const newDuration = outPoint - inPoint;
          return {
            ...c,
            inPoint,
            outPoint,
            duration: newDuration,
          };
        }),
      });
      updateDuration();
      // Invalidate RAM preview cache - content changed
      get().invalidateCache();
    },

    // Split a clip into two parts at the specified time
    splitClip: (clipId, splitTime) => {
      const { clips, updateDuration, invalidateCache } = get();
      const clip = clips.find(c => c.id === clipId);
      if (!clip) return;

      // Validate split time is within clip bounds (not at edges)
      const clipEnd = clip.startTime + clip.duration;
      if (splitTime <= clip.startTime || splitTime >= clipEnd) {
        console.warn('[Timeline] Cannot split at edge or outside clip');
        return;
      }

      // Calculate the duration of the first part
      const firstPartDuration = splitTime - clip.startTime;
      const secondPartDuration = clip.duration - firstPartDuration;

      // Calculate the split point within the source media
      const splitInSource = clip.inPoint + firstPartDuration;

      const timestamp = Date.now();
      const randomSuffix = Math.random().toString(36).substr(2, 5);

      // Create the first clip (before split point)
      const firstClip: TimelineClip = {
        ...clip,
        id: `clip-${timestamp}-${randomSuffix}-a`,
        duration: firstPartDuration,
        outPoint: splitInSource,
        // Keep linkedClipId for now, will update after creating linked clips
        linkedClipId: undefined,
      };

      // Create the second clip (after split point)
      const secondClip: TimelineClip = {
        ...clip,
        id: `clip-${timestamp}-${randomSuffix}-b`,
        startTime: splitTime,
        duration: secondPartDuration,
        inPoint: splitInSource,
        linkedClipId: undefined,
      };

      // Build new clips array starting with non-affected clips
      const newClips: TimelineClip[] = clips.filter(c => c.id !== clipId && c.id !== clip.linkedClipId);

      // Handle linked clip (e.g., audio linked to video)
      if (clip.linkedClipId) {
        const linkedClip = clips.find(c => c.id === clip.linkedClipId);
        if (linkedClip) {
          // Create split versions of linked clip
          const linkedFirstClip: TimelineClip = {
            ...linkedClip,
            id: `clip-${timestamp}-${randomSuffix}-linked-a`,
            duration: firstPartDuration,
            outPoint: linkedClip.inPoint + firstPartDuration,
            linkedClipId: firstClip.id,
          };

          const linkedSecondClip: TimelineClip = {
            ...linkedClip,
            id: `clip-${timestamp}-${randomSuffix}-linked-b`,
            startTime: splitTime,
            duration: secondPartDuration,
            inPoint: linkedClip.inPoint + firstPartDuration,
            linkedClipId: secondClip.id,
          };

          // Update the main clips to reference their linked counterparts
          firstClip.linkedClipId = linkedFirstClip.id;
          secondClip.linkedClipId = linkedSecondClip.id;

          newClips.push(linkedFirstClip, linkedSecondClip);
        }
      }

      newClips.push(firstClip, secondClip);

      set({
        clips: newClips,
        selectedClipId: secondClip.id, // Select the second clip after split
      });

      updateDuration();
      invalidateCache();
      console.log(`[Timeline] Split clip "${clip.name}" at ${splitTime.toFixed(2)}s`);
    },

    // Split the clip under the playhead (or selected clip if playhead is on it)
    splitClipAtPlayhead: () => {
      const { clips, playheadPosition, selectedClipId } = get();

      // Find clips at the current playhead position
      const clipsAtPlayhead = clips.filter(c =>
        playheadPosition > c.startTime &&
        playheadPosition < c.startTime + c.duration
      );

      if (clipsAtPlayhead.length === 0) {
        console.warn('[Timeline] No clip at playhead position');
        return;
      }

      // If selected clip is at playhead, split that one
      // Otherwise, split the topmost video clip at playhead
      let clipToSplit = clipsAtPlayhead.find(c => c.id === selectedClipId);
      if (!clipToSplit) {
        // Prefer video clips over audio clips
        clipToSplit = clipsAtPlayhead.find(c => c.source?.type === 'video' || c.source?.type === 'image');
        if (!clipToSplit) {
          clipToSplit = clipsAtPlayhead[0];
        }
      }

      get().splitClip(clipToSplit.id, playheadPosition);
    },

    selectClip: (id) => {
      set({ selectedClipId: id });
    },

    updateClipTransform: (id, transform) => {
      const { clips } = get();
      set({
        clips: clips.map(c => {
          if (c.id !== id) return c;
          return {
            ...c,
            transform: {
              ...c.transform,
              ...transform,
              position: transform.position
                ? { ...c.transform.position, ...transform.position }
                : c.transform.position,
              scale: transform.scale
                ? { ...c.transform.scale, ...transform.scale }
                : c.transform.scale,
              rotation: transform.rotation
                ? { ...c.transform.rotation, ...transform.rotation }
                : c.transform.rotation,
            },
          };
        }),
      });
      // Invalidate cache - transform affects rendered output
      get().invalidateCache();
    },

    toggleClipReverse: (id) => {
      const { clips } = get();
      set({
        clips: clips.map(c => {
          if (c.id !== id) return c;
          const newReversed = !c.reversed;
          // Reverse the thumbnails array when toggling
          const newThumbnails = c.thumbnails ? [...c.thumbnails].reverse() : c.thumbnails;
          return {
            ...c,
            reversed: newReversed,
            thumbnails: newThumbnails,
          };
        }),
      });
      // Invalidate cache - reversed playback affects rendered output
      get().invalidateCache();
    },

    // Clip effect actions
    addClipEffect: (clipId, effectType) => {
      const { clips } = get();
      const effect: Effect = {
        id: `effect_${Date.now()}`,
        name: effectType,
        type: effectType as EffectType,
        enabled: true,
        params: getDefaultEffectParams(effectType),
      };

      set({
        clips: clips.map(c =>
          c.id === clipId ? { ...c, effects: [...(c.effects || []), effect] } : c
        ),
      });
      get().invalidateCache();
    },

    removeClipEffect: (clipId, effectId) => {
      const { clips } = get();
      set({
        clips: clips.map(c =>
          c.id === clipId ? { ...c, effects: c.effects.filter(e => e.id !== effectId) } : c
        ),
      });
      get().invalidateCache();
    },

    updateClipEffect: (clipId, effectId, params) => {
      const { clips } = get();
      set({
        clips: clips.map(c =>
          c.id === clipId
            ? {
                ...c,
                effects: c.effects.map(e =>
                  e.id === effectId ? { ...e, params: { ...e.params, ...params } } : e
                ),
              }
            : c
        ),
      });
      get().invalidateCache();
    },

    // Playback actions
    setPlayheadPosition: (position) => {
      const { duration } = get();
      set({ playheadPosition: Math.max(0, Math.min(position, duration)) });
    },
    setDraggingPlayhead: (dragging) => {
      set({ isDraggingPlayhead: dragging });
    },

    play: () => {
      set({ isPlaying: true });
    },

    pause: () => {
      set({ isPlaying: false });
    },

    stop: () => {
      set({ isPlaying: false, playheadPosition: 0 });
    },

    // View actions
    setZoom: (zoom) => {
      set({ zoom: Math.max(10, Math.min(200, zoom)) });
    },

    setScrollX: (scrollX) => {
      set({ scrollX: Math.max(0, scrollX) });
    },

    // In/Out marker actions
    setInPoint: (time) => {
      const { outPoint, duration } = get();
      if (time === null) {
        set({ inPoint: null });
        return;
      }
      // Ensure in point doesn't exceed out point or duration
      const clampedTime = Math.max(0, Math.min(time, outPoint ?? duration));
      set({ inPoint: clampedTime });
    },

    setOutPoint: (time) => {
      const { inPoint, duration } = get();
      if (time === null) {
        set({ outPoint: null });
        return;
      }
      // Ensure out point doesn't precede in point and doesn't exceed duration
      const clampedTime = Math.max(inPoint ?? 0, Math.min(time, duration));
      set({ outPoint: clampedTime });
    },

    clearInOut: () => {
      set({ inPoint: null, outPoint: null });
    },

    setInPointAtPlayhead: () => {
      const { playheadPosition } = get();
      get().setInPoint(playheadPosition);
    },

    setOutPointAtPlayhead: () => {
      const { playheadPosition } = get();
      get().setOutPoint(playheadPosition);
    },

    setLoopPlayback: (loop) => {
      set({ loopPlayback: loop });
    },

    toggleLoopPlayback: () => {
      set({ loopPlayback: !get().loopPlayback });
    },

    // RAM Preview actions
    toggleRamPreviewEnabled: () => {
      const { ramPreviewEnabled } = get();
      if (ramPreviewEnabled) {
        // Turning OFF - cancel any running preview and clear cache
        set({ ramPreviewEnabled: false, isRamPreviewing: false, ramPreviewProgress: null });
        import('../engine/WebGPUEngine').then(({ engine }) => {
          engine.setGeneratingRamPreview(false);
          engine.clearCompositeCache();
        });
        set({ ramPreviewRange: null, cachedFrameTimes: new Set() });
      } else {
        // Turning ON - enable automatic RAM preview
        set({ ramPreviewEnabled: true });
      }
    },

    startRamPreview: async () => {
      const { inPoint, outPoint, duration, clips, tracks, isRamPreviewing, playheadPosition, addCachedFrame, ramPreviewEnabled } = get();
      // Don't start if RAM Preview is disabled or already running
      if (!ramPreviewEnabled || isRamPreviewing) return;

      // Determine range to preview (use In/Out or clips extent)
      const start = inPoint ?? 0;
      const end = outPoint ?? (clips.length > 0
        ? Math.max(...clips.map(c => c.startTime + c.duration))
        : duration);

      if (end <= start) return;

      // Import engine dynamically to avoid circular dependency
      const { engine } = await import('../engine/WebGPUEngine');

      // Tell engine to skip preview updates for efficiency
      engine.setGeneratingRamPreview(true);

      set({
        isRamPreviewing: true,
        ramPreviewProgress: 0,
        ramPreviewRange: null
      });

      const fps = 30; // Preview at 30fps
      const frameInterval = 1 / fps;

      // Helper: check if there's a video clip at a given time
      const hasVideoAt = (time: number) => {
        return clips.some(c =>
          time >= c.startTime &&
          time < c.startTime + c.duration &&
          (c.source?.type === 'video' || c.source?.type === 'image')
        );
      };

      // Generate frame times spreading outward from playhead
      // Only include times where there are video clips
      const centerTime = Math.max(start, Math.min(end, playheadPosition));
      const frameTimes: number[] = [];

      // Add center frame if it has video
      if (hasVideoAt(centerTime)) {
        frameTimes.push(centerTime);
      }

      // Alternate left and right from center, only adding frames with video
      let offset = frameInterval;
      while (offset <= (end - start)) {
        const rightTime = centerTime + offset;
        const leftTime = centerTime - offset;

        if (rightTime <= end && hasVideoAt(rightTime)) {
          frameTimes.push(rightTime);
        }
        if (leftTime >= start && hasVideoAt(leftTime)) {
          frameTimes.push(leftTime);
        }

        offset += frameInterval;
      }

      // No frames to render
      if (frameTimes.length === 0) {
        engine.setGeneratingRamPreview(false);
        set({ isRamPreviewing: false, ramPreviewProgress: null });
        return;
      }

      const totalFrames = frameTimes.length;
      let cancelled = false;

      // Store cancel function
      const checkCancelled = () => !get().isRamPreviewing;

      try {
        for (let frame = 0; frame < totalFrames; frame++) {
          if (checkCancelled()) {
            cancelled = true;
            break;
          }

          const time = frameTimes[frame];

          // Skip frames that are already cached (reuse existing work)
          const quantizedTime = Math.round(time * 30) / 30;
          if (get().cachedFrameTimes.has(quantizedTime)) {
            // Update progress even for skipped frames
            const progress = ((frame + 1) / totalFrames) * 100;
            set({ ramPreviewProgress: progress });
            continue;
          }

          // Get clips at this time
          const clipsAtTime = clips.filter(c =>
            time >= c.startTime && time < c.startTime + c.duration
          );

          // Build layers for this frame
          const videoTracks = tracks.filter(t => t.type === 'video');
          const layers: import('../types').Layer[] = [];

          // Seek all videos and build layers
          for (const clip of clipsAtTime) {
            const track = tracks.find(t => t.id === clip.trackId);
            if (!track?.visible || track.type !== 'video') continue;

            if (clip.source?.type === 'video' && clip.source.videoElement) {
              const video = clip.source.videoElement;
              const clipLocalTime = time - clip.startTime;
              // Handle reversed clips
              const clipTime = clip.reversed
                ? clip.outPoint - clipLocalTime
                : clipLocalTime + clip.inPoint;

              // Robust seek with verification and retry
              const seekWithVerify = async (targetTime: number, maxRetries = 3): Promise<boolean> => {
                for (let attempt = 0; attempt < maxRetries; attempt++) {
                  // Check if cancelled
                  if (checkCancelled()) return false;

                  // Seek to target time
                  await new Promise<void>((resolve) => {
                    const timeout = setTimeout(() => {
                      video.removeEventListener('seeked', onSeeked);
                      resolve();
                    }, 500);

                    const onSeeked = () => {
                      clearTimeout(timeout);
                      video.removeEventListener('seeked', onSeeked);
                      resolve();
                    };

                    video.addEventListener('seeked', onSeeked);
                    video.currentTime = targetTime;
                  });

                  // Wait for video to be fully ready (not seeking, has data)
                  await new Promise<void>((resolve) => {
                    const checkReady = () => {
                      if (!video.seeking && video.readyState >= 2) {
                        resolve();
                      } else {
                        requestAnimationFrame(checkReady);
                      }
                    };
                    checkReady();
                    // Timeout fallback
                    setTimeout(resolve, 200);
                  });

                  // Verify position is correct (within 1 frame tolerance at 30fps)
                  if (Math.abs(video.currentTime - targetTime) < 0.04) {
                    return true; // Success
                  }

                  // Position wrong (user scrubbed?), retry
                  if (checkCancelled()) return false;
                }
                return false; // Failed after retries
              };

              // Perform seek with verification
              const seekSuccess = await seekWithVerify(clipTime);
              if (!seekSuccess || checkCancelled()) {
                continue; // Skip this clip if seek failed or cancelled
              }

              // Add to layers
              layers.push({
                id: clip.id,
                name: clip.name,
                visible: true,
                opacity: clip.transform.opacity,
                blendMode: clip.transform.blendMode,
                source: { type: 'video', videoElement: video },
                effects: [],
                position: { x: clip.transform.position.x, y: clip.transform.position.y },
                scale: { x: clip.transform.scale.x, y: clip.transform.scale.y },
                rotation: { x: clip.transform.rotation.x * (Math.PI / 180), y: clip.transform.rotation.y * (Math.PI / 180), z: clip.transform.rotation.z * (Math.PI / 180) },
              });
            } else if (clip.source?.type === 'image' && clip.source.imageElement) {
              layers.push({
                id: clip.id,
                name: clip.name,
                visible: true,
                opacity: clip.transform.opacity,
                blendMode: clip.transform.blendMode,
                source: { type: 'image', imageElement: clip.source.imageElement },
                effects: [],
                position: { x: clip.transform.position.x, y: clip.transform.position.y },
                scale: { x: clip.transform.scale.x, y: clip.transform.scale.y },
                rotation: { x: clip.transform.rotation.x * (Math.PI / 180), y: clip.transform.rotation.y * (Math.PI / 180), z: clip.transform.rotation.z * (Math.PI / 180) },
              });
            }
          }

          // Sort layers by track order
          const trackOrder = new Map(videoTracks.map((t, i) => [t.id, i]));
          layers.sort((a, b) => {
            const clipA = clipsAtTime.find(c => c.id === a.id);
            const clipB = clipsAtTime.find(c => c.id === b.id);
            const orderA = clipA ? (trackOrder.get(clipA.trackId) ?? 0) : 0;
            const orderB = clipB ? (trackOrder.get(clipB.trackId) ?? 0) : 0;
            return orderA - orderB;
          });

          // Final verification: ensure all videos are still at correct position before rendering
          // This catches cases where user interaction changed position between seek and render
          let allPositionsCorrect = true;
          for (const clip of clipsAtTime) {
            if (clip.source?.type === 'video' && clip.source.videoElement) {
              const video = clip.source.videoElement;
              const localTime = time - clip.startTime;
              const expectedTime = clip.reversed
                ? clip.outPoint - localTime
                : localTime + clip.inPoint;
              if (Math.abs(video.currentTime - expectedTime) > 0.04) {
                allPositionsCorrect = false;
                break;
              }
            }
          }

          // Skip this frame if positions are wrong (user scrubbed) or cancelled
          if (!allPositionsCorrect || checkCancelled()) {
            continue;
          }

          // Render and cache this frame
          if (layers.length > 0) {
            engine.render(layers);
          }
          await engine.cacheCompositeFrame(time);

          // Add to cached frames set (shows green indicator immediately)
          addCachedFrame(time);

          // Update progress percentage
          const progress = ((frame + 1) / totalFrames) * 100;
          set({ ramPreviewProgress: progress });

          // Yield to allow UI updates (every frame for smooth green dot updates)
          if (frame % 3 === 0) {
            await new Promise(resolve => setTimeout(resolve, 0));
          }
        }

        if (!cancelled) {
          // Set the preview range for cache hit detection
          set({
            ramPreviewRange: { start, end },
            ramPreviewProgress: null
          });
        }
      } catch (error) {
        console.error('[RAM Preview] Error:', error);
      } finally {
        engine.setGeneratingRamPreview(false);
        set({ isRamPreviewing: false, ramPreviewProgress: null });
      }
    },

    cancelRamPreview: () => {
      // IMMEDIATELY set state to cancel the loop - this must be synchronous!
      // The RAM preview loop checks !get().isRamPreviewing to know when to stop
      set({ isRamPreviewing: false, ramPreviewProgress: null });
      // Then async cleanup the engine
      import('../engine/WebGPUEngine').then(({ engine }) => {
        engine.setGeneratingRamPreview(false);
      });
    },

    clearRamPreview: async () => {
      const { engine } = await import('../engine/WebGPUEngine');
      engine.clearCompositeCache();
      set({ ramPreviewRange: null, ramPreviewProgress: null, cachedFrameTimes: new Set() });
    },

    // Playback frame caching (green line like After Effects)
    addCachedFrame: (time: number) => {
      const quantized = Math.round(time * 30) / 30; // Quantize to 30fps
      const { cachedFrameTimes } = get();
      if (!cachedFrameTimes.has(quantized)) {
        const newSet = new Set(cachedFrameTimes);
        newSet.add(quantized);
        set({ cachedFrameTimes: newSet });
      }
    },

    getCachedRanges: () => {
      const { cachedFrameTimes } = get();
      if (cachedFrameTimes.size === 0) return [];

      // Convert set to sorted array
      const times = Array.from(cachedFrameTimes).sort((a, b) => a - b);
      const ranges: Array<{ start: number; end: number }> = [];
      const frameInterval = 1 / 30;
      const gap = frameInterval * 2; // Allow gap of 2 frames

      let rangeStart = times[0];
      let rangeEnd = times[0];

      for (let i = 1; i < times.length; i++) {
        if (times[i] - rangeEnd <= gap) {
          // Continue range
          rangeEnd = times[i];
        } else {
          // End range and start new one
          ranges.push({ start: rangeStart, end: rangeEnd + frameInterval });
          rangeStart = times[i];
          rangeEnd = times[i];
        }
      }

      // Add final range
      ranges.push({ start: rangeStart, end: rangeEnd + frameInterval });

      return ranges;
    },

    // Invalidate cache when content changes (clip moved, trimmed, etc.)
    invalidateCache: () => {
      // Cancel any ongoing RAM preview
      set({ isRamPreviewing: false });
      // Clear the cache
      import('../engine/WebGPUEngine').then(({ engine }) => {
        engine.setGeneratingRamPreview(false);
        engine.clearCompositeCache();
      });
      // Clear cached frame times
      set({ cachedFrameTimes: new Set(), ramPreviewRange: null, ramPreviewProgress: null });
    },

    // Utils
    getClipsAtTime: (time) => {
      const { clips } = get();
      return clips.filter(c => time >= c.startTime && time < c.startTime + c.duration);
    },

    updateDuration: () => {
      const { clips } = get();
      if (clips.length === 0) {
        set({ duration: 60 });
        return;
      }
      const maxEnd = Math.max(...clips.map(c => c.startTime + c.duration));
      set({ duration: Math.max(60, maxEnd + 10) }); // Add 10 seconds padding
    },

    findAvailableAudioTrack: (startTime: number, duration: number) => {
      const { tracks, clips, addTrack } = get();
      const audioTracks = tracks.filter(t => t.type === 'audio');
      const endTime = startTime + duration;

      // Check each audio track for availability
      for (const track of audioTracks) {
        const trackClips = clips.filter(c => c.trackId === track.id);
        const hasOverlap = trackClips.some(clip => {
          const clipEnd = clip.startTime + clip.duration;
          // Check if time ranges overlap
          return !(endTime <= clip.startTime || startTime >= clipEnd);
        });

        if (!hasOverlap) {
          return track.id; // This track is available
        }
      }

      // No available audio track found, create a new one
      addTrack('audio');
      const { tracks: updatedTracks } = get();
      const newTrack = updatedTracks[updatedTracks.length - 1];
      console.log('[Timeline] Created new audio track:', newTrack.name);
      return newTrack.id;
    },

    // Get snapped position - snaps to edges of other clips on the same track
    getSnappedPosition: (clipId: string, desiredStartTime: number, trackId: string) => {
      const { clips } = get();
      const movingClip = clips.find(c => c.id === clipId);
      if (!movingClip) return { startTime: desiredStartTime, snapped: false };

      const clipDuration = movingClip.duration;
      const desiredEndTime = desiredStartTime + clipDuration;

      // Get other clips on the same track (excluding the moving clip and its linked clip)
      const otherClips = clips.filter(c =>
        c.trackId === trackId &&
        c.id !== clipId &&
        c.id !== movingClip.linkedClipId &&
        c.linkedClipId !== clipId
      );

      let snappedStart = desiredStartTime;
      let snapped = false;
      let minSnapDistance = SNAP_THRESHOLD_SECONDS;

      // Check snap points
      for (const clip of otherClips) {
        const clipEnd = clip.startTime + clip.duration;

        // Snap start of moving clip to end of other clip
        const distToEnd = Math.abs(desiredStartTime - clipEnd);
        if (distToEnd < minSnapDistance) {
          snappedStart = clipEnd;
          minSnapDistance = distToEnd;
          snapped = true;
        }

        // Snap start of moving clip to start of other clip
        const distToStart = Math.abs(desiredStartTime - clip.startTime);
        if (distToStart < minSnapDistance) {
          snappedStart = clip.startTime;
          minSnapDistance = distToStart;
          snapped = true;
        }

        // Snap end of moving clip to start of other clip
        const distEndToStart = Math.abs(desiredEndTime - clip.startTime);
        if (distEndToStart < minSnapDistance) {
          snappedStart = clip.startTime - clipDuration;
          minSnapDistance = distEndToStart;
          snapped = true;
        }

        // Snap end of moving clip to end of other clip
        const distEndToEnd = Math.abs(desiredEndTime - clipEnd);
        if (distEndToEnd < minSnapDistance) {
          snappedStart = clipEnd - clipDuration;
          minSnapDistance = distEndToEnd;
          snapped = true;
        }
      }

      // Also snap to timeline start (0)
      if (Math.abs(desiredStartTime) < SNAP_THRESHOLD_SECONDS) {
        snappedStart = 0;
        snapped = true;
      }

      return { startTime: Math.max(0, snappedStart), snapped };
    },

    // Find a valid non-overlapping position for a clip
    findNonOverlappingPosition: (clipId: string, desiredStartTime: number, trackId: string, duration: number) => {
      const { clips } = get();
      const movingClip = clips.find(c => c.id === clipId);

      // Get other clips on the same track (excluding the moving clip and its linked clip)
      const otherClips = clips.filter(c =>
        c.trackId === trackId &&
        c.id !== clipId &&
        (movingClip ? c.id !== movingClip.linkedClipId && c.linkedClipId !== clipId : true)
      ).sort((a, b) => a.startTime - b.startTime);

      const desiredEndTime = desiredStartTime + duration;

      // Check if desired position overlaps with any clip
      let overlappingClip: TimelineClip | null = null;
      for (const clip of otherClips) {
        const clipEnd = clip.startTime + clip.duration;
        // Check if time ranges overlap
        if (!(desiredEndTime <= clip.startTime || desiredStartTime >= clipEnd)) {
          overlappingClip = clip;
          break;
        }
      }

      // If no overlap, use desired position
      if (!overlappingClip) {
        return Math.max(0, desiredStartTime);
      }

      // There's an overlap - push clip to the nearest edge
      const overlappingEnd = overlappingClip.startTime + overlappingClip.duration;

      // Check which side is closer
      const distToStart = Math.abs(desiredStartTime - overlappingClip.startTime);
      const distToEnd = Math.abs(desiredStartTime - overlappingEnd);

      if (distToStart < distToEnd) {
        // Push to left side (end at overlapping clip's start)
        const newStart = overlappingClip.startTime - duration;

        // Check if this position overlaps with another clip
        const wouldOverlap = otherClips.some(c => {
          if (c.id === overlappingClip!.id) return false;
          const cEnd = c.startTime + c.duration;
          const newEnd = newStart + duration;
          return !(newEnd <= c.startTime || newStart >= cEnd);
        });

        if (!wouldOverlap && newStart >= 0) {
          return newStart;
        }
      }

      // Push to right side (start at overlapping clip's end)
      const newStart = overlappingEnd;

      // Check if this position overlaps with another clip
      const wouldOverlap = otherClips.some(c => {
        if (c.id === overlappingClip!.id) return false;
        const cEnd = c.startTime + c.duration;
        const newEnd = newStart + duration;
        return !(newEnd <= c.startTime || newStart >= cEnd);
      });

      if (!wouldOverlap) {
        return newStart;
      }

      // As a fallback, return the desired position (shouldn't happen often)
      return Math.max(0, desiredStartTime);
    },

    // Get serializable timeline state for saving to composition
    getSerializableState: (): CompositionTimelineData => {
      const { tracks, clips, playheadPosition, duration, zoom, scrollX, inPoint, outPoint, loopPlayback, clipKeyframes } = get();

      // Convert clips to serializable format (without DOM elements)
      const mediaStore = useMediaStore.getState();
      const serializableClips: SerializableClip[] = clips.map(clip => {
        // Find the mediaFile ID by matching the file name in mediaStore
        // For linked audio clips (name ends with "(Audio)"), strip the suffix to find the video file
        let lookupName = clip.name;
        if (clip.linkedClipId && clip.source?.type === 'audio' && lookupName.endsWith(' (Audio)')) {
          lookupName = lookupName.replace(' (Audio)', '');
        }
        const mediaFile = mediaStore.files.find(f => f.name === lookupName);

        // Get keyframes for this clip
        const keyframes = clipKeyframes.get(clip.id) || [];

        return {
          id: clip.id,
          trackId: clip.trackId,
          name: clip.name,
          mediaFileId: clip.isComposition ? '' : (mediaFile?.id || ''), // Comp clips don't have media files
          startTime: clip.startTime,
          duration: clip.duration,
          inPoint: clip.inPoint,
          outPoint: clip.outPoint,
          sourceType: clip.source?.type || 'video',
          naturalDuration: clip.source?.naturalDuration,
          thumbnails: clip.thumbnails,
          linkedClipId: clip.linkedClipId,
          waveform: clip.waveform,
          transform: clip.transform,
          keyframes: keyframes.length > 0 ? keyframes : undefined,
          // Nested composition support
          isComposition: clip.isComposition,
          compositionId: clip.compositionId,
          // Mask support
          masks: clip.masks && clip.masks.length > 0 ? clip.masks : undefined,
        };
      });

      return {
        tracks,
        clips: serializableClips,
        playheadPosition,
        duration,
        zoom,
        scrollX,
        inPoint,
        outPoint,
        loopPlayback,
      };
    },

    // Load timeline state from composition data
    loadState: async (data: CompositionTimelineData | undefined) => {
      const { pause, clearTimeline } = get();

      // Stop playback
      pause();

      // Clear current timeline
      clearTimeline();

      if (!data) {
        // No data - start with fresh default timeline
        set({
          tracks: DEFAULT_TRACKS.map(t => ({ ...t })),
          clips: [],
          playheadPosition: 0,
          duration: 60,
          zoom: 50,
          scrollX: 0,
          inPoint: null,
          outPoint: null,
          loopPlayback: false,
          selectedClipId: null,
        });
        return;
      }

      // Restore tracks and basic state
      set({
        tracks: data.tracks.map(t => ({ ...t })),
        clips: [], // We'll restore clips separately
        playheadPosition: data.playheadPosition,
        duration: data.duration,
        zoom: data.zoom,
        scrollX: data.scrollX,
        inPoint: data.inPoint,
        outPoint: data.outPoint,
        loopPlayback: data.loopPlayback,
        selectedClipId: null,
        // Clear keyframe state
        clipKeyframes: new Map<string, Keyframe[]>(),
        keyframeRecordingEnabled: new Set<string>(),
        expandedTracks: new Set<string>(),
        expandedTrackPropertyGroups: new Map<string, Set<string>>(),
        selectedKeyframeIds: new Set<string>(),
      });

      // Restore keyframes from serialized clips
      const keyframeMap = new Map<string, Keyframe[]>();
      for (const serializedClip of data.clips) {
        if (serializedClip.keyframes && serializedClip.keyframes.length > 0) {
          keyframeMap.set(serializedClip.id, serializedClip.keyframes);
        }
      }
      if (keyframeMap.size > 0) {
        set({ clipKeyframes: keyframeMap });
      }

      // Restore clips - need to recreate media elements from file references
      const mediaStore = useMediaStore.getState();

      for (const serializedClip of data.clips) {
        // Handle composition clips specially
        if (serializedClip.isComposition && serializedClip.compositionId) {
          const composition = mediaStore.compositions.find(c => c.id === serializedClip.compositionId);
          if (composition) {
            // Use addCompClip to properly load nested composition
            // But we need to restore specific settings, so create clip manually first
            const compClip: TimelineClip = {
              id: serializedClip.id,
              trackId: serializedClip.trackId,
              name: serializedClip.name,
              file: new File([], serializedClip.name),
              startTime: serializedClip.startTime,
              duration: serializedClip.duration,
              inPoint: serializedClip.inPoint,
              outPoint: serializedClip.outPoint,
              source: {
                type: 'video',
                naturalDuration: serializedClip.duration,
              },
              thumbnails: serializedClip.thumbnails,
              transform: serializedClip.transform,
              effects: serializedClip.effects || [],
              isLoading: true,
              isComposition: true,
              compositionId: serializedClip.compositionId,
              nestedClips: [],
              nestedTracks: [],
            };

            // Add clip to state
            set(state => ({
              clips: [...state.clips, compClip],
            }));

            // Load nested composition content in background
            if (composition.timelineData) {
              const nestedClips: TimelineClip[] = [];
              const nestedTracks = composition.timelineData.tracks;

              for (const nestedSerializedClip of composition.timelineData.clips) {
                const nestedMediaFile = mediaStore.files.find(f => f.id === nestedSerializedClip.mediaFileId);
                if (!nestedMediaFile || !nestedMediaFile.file) continue;

                const nestedClip: TimelineClip = {
                  id: `nested-${compClip.id}-${nestedSerializedClip.id}`,
                  trackId: nestedSerializedClip.trackId,
                  name: nestedSerializedClip.name,
                  file: nestedMediaFile.file,
                  startTime: nestedSerializedClip.startTime,
                  duration: nestedSerializedClip.duration,
                  inPoint: nestedSerializedClip.inPoint,
                  outPoint: nestedSerializedClip.outPoint,
                  source: null,
                  thumbnails: nestedSerializedClip.thumbnails,
                  transform: nestedSerializedClip.transform,
                  effects: nestedSerializedClip.effects || [],
                  isLoading: true,
                };

                nestedClips.push(nestedClip);

                // Load media element
                const nestedType = nestedSerializedClip.sourceType;
                const nestedFileUrl = URL.createObjectURL(nestedMediaFile.file);

                if (nestedType === 'video') {
                  const video = document.createElement('video');
                  video.src = nestedFileUrl;
                  video.muted = true;
                  video.playsInline = true;
                  video.preload = 'auto';
                  video.crossOrigin = 'anonymous';

                  video.addEventListener('canplaythrough', () => {
                    nestedClip.source = {
                      type: 'video',
                      videoElement: video,
                      naturalDuration: video.duration,
                    };
                    nestedClip.isLoading = false;
                    // Trigger update
                    const currentClips = get().clips;
                    set({ clips: [...currentClips] });
                  }, { once: true });
                } else if (nestedType === 'image') {
                  const img = new Image();
                  img.src = nestedFileUrl;
                  img.addEventListener('load', () => {
                    nestedClip.source = {
                      type: 'image',
                      imageElement: img,
                    };
                    nestedClip.isLoading = false;
                    const currentClips = get().clips;
                    set({ clips: [...currentClips] });
                  }, { once: true });
                }
              }

              // Update comp clip with nested data
              set(state => ({
                clips: state.clips.map(c =>
                  c.id === compClip.id
                    ? { ...c, nestedClips, nestedTracks, isLoading: false }
                    : c
                ),
              }));
            } else {
              // No timeline data
              set(state => ({
                clips: state.clips.map(c =>
                  c.id === compClip.id ? { ...c, isLoading: false } : c
                ),
              }));
            }
          } else {
            console.warn('Could not find composition for clip:', serializedClip.name);
          }
          continue;
        }

        // Regular media clips
        const mediaFile = mediaStore.files.find(f => f.id === serializedClip.mediaFileId);
        if (!mediaFile || !mediaFile.file) {
          console.warn('Could not find media file for clip:', serializedClip.name);
          continue;
        }

        // Add the clip using the existing addClip function which handles media loading
        // But we need to restore the specific settings, so we'll do it manually
        const clip: TimelineClip = {
          id: serializedClip.id,
          trackId: serializedClip.trackId,
          name: serializedClip.name,
          file: mediaFile.file,
          startTime: serializedClip.startTime,
          duration: serializedClip.duration,
          inPoint: serializedClip.inPoint,
          outPoint: serializedClip.outPoint,
          source: null, // Will be loaded
          thumbnails: serializedClip.thumbnails,
          linkedClipId: serializedClip.linkedClipId,
          waveform: serializedClip.waveform,
          transform: serializedClip.transform,
          effects: serializedClip.effects || [],
          isLoading: true,
          masks: serializedClip.masks,  // Restore masks
        };

        // Add clip to state
        set(state => ({
          clips: [...state.clips, clip],
        }));

        // Load media element async
        const type = serializedClip.sourceType;
        const fileUrl = URL.createObjectURL(mediaFile.file);

        if (type === 'video') {
          const video = document.createElement('video');
          video.src = fileUrl;
          video.muted = true;
          video.playsInline = true;
          video.preload = 'auto';

          video.addEventListener('canplaythrough', () => {
            set(state => ({
              clips: state.clips.map(c =>
                c.id === clip.id
                  ? {
                      ...c,
                      source: {
                        type: 'video',
                        videoElement: video,
                        naturalDuration: video.duration,
                      },
                      isLoading: false,
                    }
                  : c
              ),
            }));
          }, { once: true });
        } else if (type === 'audio') {
          // Audio clips - create audio element (works for both pure audio files and linked audio from video)
          const audio = document.createElement('audio');
          audio.src = fileUrl;
          audio.preload = 'auto';

          audio.addEventListener('canplaythrough', () => {
            set(state => ({
              clips: state.clips.map(c =>
                c.id === clip.id
                  ? {
                      ...c,
                      source: {
                        type: 'audio',
                        audioElement: audio,
                        naturalDuration: audio.duration,
                      },
                      isLoading: false,
                    }
                  : c
              ),
            }));
          }, { once: true });
        } else if (type === 'image') {
          const img = new Image();
          img.src = fileUrl;

          img.addEventListener('load', () => {
            set(state => ({
              clips: state.clips.map(c =>
                c.id === clip.id
                  ? {
                      ...c,
                      source: { type: 'image', imageElement: img },
                      isLoading: false,
                    }
                  : c
              ),
            }));
          }, { once: true });
        }
      }
    },

    // Clear all timeline data
    clearTimeline: () => {
      const { clips, pause } = get();

      // Stop playback
      pause();

      // Clean up media elements
      clips.forEach(clip => {
        if (clip.source?.videoElement) {
          clip.source.videoElement.pause();
          clip.source.videoElement.src = '';
        }
        if (clip.source?.audioElement) {
          clip.source.audioElement.pause();
          clip.source.audioElement.src = '';
        }
        if (clip.source?.webCodecsPlayer) {
          clip.source.webCodecsPlayer.destroy();
        }
      });

      // Clear mixer store layers so preview shows black
      useMixerStore.setState({ layers: [] });

      set({
        clips: [],
        selectedClipId: null,
        cachedFrameTimes: new Set(),
        ramPreviewProgress: null,
        ramPreviewRange: null,
        isRamPreviewing: false,
        // Clear keyframe state
        clipKeyframes: new Map<string, Keyframe[]>(),
        keyframeRecordingEnabled: new Set<string>(),
        expandedTracks: new Set<string>(),
        expandedTrackPropertyGroups: new Map<string, Set<string>>(),
        selectedKeyframeIds: new Set<string>(),
      });
    },

    // Keyframe actions
    addKeyframe: (clipId, property, value, time, easing = 'linear') => {
      const { clips, playheadPosition, clipKeyframes, invalidateCache } = get();
      const clip = clips.find(c => c.id === clipId);
      if (!clip) return;

      // Calculate time relative to clip start
      const clipLocalTime = time ?? (playheadPosition - clip.startTime);

      // Clamp to clip duration
      const clampedTime = Math.max(0, Math.min(clipLocalTime, clip.duration));

      // Get existing keyframes for this clip
      const existingKeyframes = clipKeyframes.get(clipId) || [];

      // Check if keyframe already exists at this time for this property
      const existingAtTime = getKeyframeAtTime(existingKeyframes, property, clampedTime);

      let newKeyframes: Keyframe[];

      if (existingAtTime) {
        // Update existing keyframe
        newKeyframes = existingKeyframes.map(k =>
          k.id === existingAtTime.id ? { ...k, value, easing } : k
        );
      } else {
        // Create new keyframe
        const newKeyframe: Keyframe = {
          id: `kf_${Date.now()}_${Math.random().toString(36).substr(2, 5)}`,
          clipId,
          time: clampedTime,
          property,
          value,
          easing,
        };
        newKeyframes = [...existingKeyframes, newKeyframe].sort((a, b) => a.time - b.time);
      }

      // Update state
      const newMap = new Map(clipKeyframes);
      newMap.set(clipId, newKeyframes);
      set({ clipKeyframes: newMap });

      // Invalidate cache since animation changed
      invalidateCache();
    },

    removeKeyframe: (keyframeId) => {
      const { clipKeyframes, invalidateCache, selectedKeyframeIds } = get();
      const newMap = new Map<string, Keyframe[]>();

      clipKeyframes.forEach((keyframes, clipId) => {
        const filtered = keyframes.filter(k => k.id !== keyframeId);
        if (filtered.length > 0) {
          newMap.set(clipId, filtered);
        }
      });

      // Remove from selection
      const newSelection = new Set(selectedKeyframeIds);
      newSelection.delete(keyframeId);

      set({ clipKeyframes: newMap, selectedKeyframeIds: newSelection });
      invalidateCache();
    },

    updateKeyframe: (keyframeId, updates) => {
      const { clipKeyframes, invalidateCache } = get();
      const newMap = new Map<string, Keyframe[]>();

      clipKeyframes.forEach((keyframes, clipId) => {
        newMap.set(clipId, keyframes.map(k =>
          k.id === keyframeId ? { ...k, ...updates } : k
        ));
      });

      set({ clipKeyframes: newMap });
      invalidateCache();
    },

    moveKeyframe: (keyframeId, newTime) => {
      const { clipKeyframes, clips, invalidateCache } = get();
      const newMap = new Map<string, Keyframe[]>();

      clipKeyframes.forEach((keyframes, clipId) => {
        const clip = clips.find(c => c.id === clipId);
        const maxTime = clip?.duration ?? 999;

        newMap.set(clipId, keyframes.map(k => {
          if (k.id !== keyframeId) return k;
          return { ...k, time: Math.max(0, Math.min(newTime, maxTime)) };
        }).sort((a, b) => a.time - b.time));
      });

      set({ clipKeyframes: newMap });
      invalidateCache();
    },

    getClipKeyframes: (clipId) => {
      const { clipKeyframes } = get();
      return clipKeyframes.get(clipId) || [];
    },

    getInterpolatedTransform: (clipId, clipLocalTime) => {
      const { clips, clipKeyframes } = get();
      const clip = clips.find(c => c.id === clipId);
      if (!clip) {
        return { ...DEFAULT_TRANSFORM };
      }

      const keyframes = clipKeyframes.get(clipId) || [];
      if (keyframes.length === 0) {
        return clip.transform;
      }

      return getInterpolatedClipTransform(keyframes, clipLocalTime, clip.transform);
    },

    getInterpolatedEffects: (clipId, clipLocalTime) => {
      const { clips, clipKeyframes } = get();
      const clip = clips.find(c => c.id === clipId);
      if (!clip || !clip.effects) {
        return [];
      }

      const keyframes = clipKeyframes.get(clipId) || [];
      if (keyframes.length === 0) {
        return clip.effects;
      }

      // Filter keyframes that are effect keyframes
      const effectKeyframes = keyframes.filter(k => k.property.startsWith('effect.'));

      if (effectKeyframes.length === 0) {
        return clip.effects;
      }

      // Clone effects and apply interpolated values
      return clip.effects.map(effect => {
        const newParams = { ...effect.params };

        // Check each numeric parameter for keyframes
        Object.keys(effect.params).forEach(paramName => {
          if (typeof effect.params[paramName] !== 'number') return;

          const propertyKey = `effect.${effect.id}.${paramName}`;
          const paramKeyframes = effectKeyframes.filter(k => k.property === propertyKey);

          if (paramKeyframes.length > 0) {
            // Interpolate the value
            newParams[paramName] = interpolateKeyframes(
              keyframes,
              propertyKey as AnimatableProperty,
              clipLocalTime,
              effect.params[paramName] as number
            );
          }
        });

        return { ...effect, params: newParams };
      });
    },

    hasKeyframes: (clipId, property) => {
      const { clipKeyframes } = get();
      const keyframes = clipKeyframes.get(clipId) || [];
      if (keyframes.length === 0) return false;
      if (!property) return true;
      return hasKeyframesForProperty(keyframes, property);
    },

    // Keyframe recording mode
    toggleKeyframeRecording: (clipId, property) => {
      const { keyframeRecordingEnabled } = get();
      const key = `${clipId}:${property}`;
      const newSet = new Set(keyframeRecordingEnabled);

      if (newSet.has(key)) {
        newSet.delete(key);
      } else {
        newSet.add(key);
      }

      set({ keyframeRecordingEnabled: newSet });
    },

    isRecording: (clipId, property) => {
      const { keyframeRecordingEnabled } = get();
      return keyframeRecordingEnabled.has(`${clipId}:${property}`);
    },

    setPropertyValue: (clipId, property, value) => {
      const { isRecording, addKeyframe, updateClipTransform, updateClipEffect, clips, hasKeyframes } = get();

      // Check if this property has keyframes (whether recording or not)
      const propertyHasKeyframes = hasKeyframes(clipId, property);

      if (isRecording(clipId, property) || propertyHasKeyframes) {
        // Recording mode OR property already has keyframes - create/update keyframe
        addKeyframe(clipId, property, value);
      } else {
        // Not recording and no keyframes - update static value
        const clip = clips.find(c => c.id === clipId);
        if (!clip) return;

        // Handle effect properties (format: effect.{effectId}.{paramName})
        if (property.startsWith('effect.')) {
          const parts = property.split('.');
          if (parts.length === 3) {
            const effectId = parts[1];
            const paramName = parts[2];
            updateClipEffect(clipId, effectId, { [paramName]: value });
          }
          return;
        }

        // Build partial transform update from property path
        const transformUpdate: Partial<ClipTransform> = {};

        if (property === 'opacity') {
          transformUpdate.opacity = value;
        } else if (property.startsWith('position.')) {
          const axis = property.split('.')[1] as 'x' | 'y' | 'z';
          transformUpdate.position = { ...clip.transform.position, [axis]: value };
        } else if (property.startsWith('scale.')) {
          const axis = property.split('.')[1] as 'x' | 'y';
          transformUpdate.scale = { ...clip.transform.scale, [axis]: value };
        } else if (property.startsWith('rotation.')) {
          const axis = property.split('.')[1] as 'x' | 'y' | 'z';
          transformUpdate.rotation = { ...clip.transform.rotation, [axis]: value };
        }

        updateClipTransform(clipId, transformUpdate);
      }
    },

    // Keyframe UI state - Track-based expansion
    toggleTrackExpanded: (trackId) => {
      const { expandedTracks } = get();
      const newSet = new Set(expandedTracks);

      if (newSet.has(trackId)) {
        newSet.delete(trackId);
      } else {
        newSet.add(trackId);
      }

      set({ expandedTracks: newSet });
    },

    isTrackExpanded: (trackId) => {
      const { expandedTracks } = get();
      return expandedTracks.has(trackId);
    },

    toggleTrackPropertyGroupExpanded: (trackId, groupName) => {
      const { expandedTrackPropertyGroups } = get();
      const newMap = new Map(expandedTrackPropertyGroups);
      const trackGroups = newMap.get(trackId) || new Set<string>();
      const newTrackGroups = new Set(trackGroups);

      if (newTrackGroups.has(groupName)) {
        newTrackGroups.delete(groupName);
      } else {
        newTrackGroups.add(groupName);
      }

      newMap.set(trackId, newTrackGroups);
      set({ expandedTrackPropertyGroups: newMap });
    },

    isTrackPropertyGroupExpanded: (trackId, groupName) => {
      const { expandedTrackPropertyGroups } = get();
      const trackGroups = expandedTrackPropertyGroups.get(trackId);
      return trackGroups?.has(groupName) ?? false;
    },

    // Calculate expanded track height based on visible property rows
    getExpandedTrackHeight: (trackId, baseHeight) => {
      const { expandedTracks, expandedTrackPropertyGroups, clips, selectedClipId, clipKeyframes } = get();

      if (!expandedTracks.has(trackId)) {
        return baseHeight;
      }

      // Get the selected clip in this track
      const trackClips = clips.filter(c => c.trackId === trackId);
      const selectedTrackClip = trackClips.find(c => c.id === selectedClipId);

      // If no clip is selected in this track, no property rows
      if (!selectedTrackClip) {
        return baseHeight;
      }

      const clipId = selectedTrackClip.id;
      const keyframes = clipKeyframes.get(clipId) || [];

      // Helper to check if a property has keyframes
      const propertyHasKeyframes = (property: string): boolean => {
        return keyframes.some(k => k.property === property);
      };

      // Check which property groups have keyframes
      const hasOpacityKeyframes = propertyHasKeyframes('opacity');
      const hasPositionXKeyframes = propertyHasKeyframes('position.x');
      const hasPositionYKeyframes = propertyHasKeyframes('position.y');
      const hasPositionZKeyframes = propertyHasKeyframes('position.z');
      const hasPositionKeyframes = hasPositionXKeyframes || hasPositionYKeyframes || hasPositionZKeyframes;
      const hasScaleXKeyframes = propertyHasKeyframes('scale.x');
      const hasScaleYKeyframes = propertyHasKeyframes('scale.y');
      const hasScaleKeyframes = hasScaleXKeyframes || hasScaleYKeyframes;
      const hasRotationXKeyframes = propertyHasKeyframes('rotation.x');
      const hasRotationYKeyframes = propertyHasKeyframes('rotation.y');
      const hasRotationZKeyframes = propertyHasKeyframes('rotation.z');
      const hasRotationKeyframes = hasRotationXKeyframes || hasRotationYKeyframes || hasRotationZKeyframes;

      // Check for effect keyframes
      const effectsWithKeyframes = selectedTrackClip.effects?.filter(effect => {
        const numericParams = Object.keys(effect.params).filter(k => typeof effect.params[k] === 'number');
        return numericParams.some(paramName => propertyHasKeyframes(`effect.${effect.id}.${paramName}`));
      }) || [];

      // If no keyframes at all, no property rows
      if (!hasOpacityKeyframes && !hasPositionKeyframes && !hasScaleKeyframes && !hasRotationKeyframes && effectsWithKeyframes.length === 0) {
        return baseHeight;
      }

      const PROPERTY_ROW_HEIGHT = 18;
      const GROUP_HEADER_HEIGHT = 20;
      let extraHeight = 0;
      const trackGroups = expandedTrackPropertyGroups.get(trackId);

      // Opacity row (only if has keyframes)
      if (hasOpacityKeyframes) {
        extraHeight += PROPERTY_ROW_HEIGHT;
      }

      // Position group (only if has keyframes)
      if (hasPositionKeyframes) {
        extraHeight += GROUP_HEADER_HEIGHT;
        if (trackGroups?.has('position')) {
          if (hasPositionXKeyframes) extraHeight += PROPERTY_ROW_HEIGHT;
          if (hasPositionYKeyframes) extraHeight += PROPERTY_ROW_HEIGHT;
          if (hasPositionZKeyframes) extraHeight += PROPERTY_ROW_HEIGHT;
        }
      }

      // Scale group (only if has keyframes)
      if (hasScaleKeyframes) {
        extraHeight += GROUP_HEADER_HEIGHT;
        if (trackGroups?.has('scale')) {
          if (hasScaleXKeyframes) extraHeight += PROPERTY_ROW_HEIGHT;
          if (hasScaleYKeyframes) extraHeight += PROPERTY_ROW_HEIGHT;
        }
      }

      // Rotation group (only if has keyframes)
      if (hasRotationKeyframes) {
        extraHeight += GROUP_HEADER_HEIGHT;
        if (trackGroups?.has('rotation')) {
          if (hasRotationXKeyframes) extraHeight += PROPERTY_ROW_HEIGHT;
          if (hasRotationYKeyframes) extraHeight += PROPERTY_ROW_HEIGHT;
          if (hasRotationZKeyframes) extraHeight += PROPERTY_ROW_HEIGHT;
        }
      }

      // Effects group - only effects that have keyframes
      if (effectsWithKeyframes.length > 0) {
        extraHeight += GROUP_HEADER_HEIGHT; // Effects group header

        if (trackGroups?.has('effects')) {
          // Add height for each effect with keyframes
          for (const effect of effectsWithKeyframes) {
            extraHeight += GROUP_HEADER_HEIGHT; // Effect sub-group header

            // If effect is expanded, add rows for each keyframed parameter
            if (trackGroups?.has(`effect.${effect.id}`)) {
              const paramsWithKeyframes = Object.keys(effect.params)
                .filter(k => typeof effect.params[k] === 'number')
                .filter(paramName => propertyHasKeyframes(`effect.${effect.id}.${paramName}`));
              extraHeight += PROPERTY_ROW_HEIGHT * paramsWithKeyframes.length;
            }
          }
        }
      }

      return baseHeight + extraHeight;
    },

    // Check if any clip on a track has keyframes
    trackHasKeyframes: (trackId) => {
      const { clips, clipKeyframes } = get();
      const trackClips = clips.filter(c => c.trackId === trackId);
      return trackClips.some(clip => {
        const kfs = clipKeyframes.get(clip.id);
        return kfs && kfs.length > 0;
      });
    },

    selectKeyframe: (keyframeId, addToSelection = false) => {
      const { selectedKeyframeIds } = get();

      if (addToSelection) {
        const newSet = new Set(selectedKeyframeIds);
        if (newSet.has(keyframeId)) {
          newSet.delete(keyframeId);
        } else {
          newSet.add(keyframeId);
        }
        set({ selectedKeyframeIds: newSet });
      } else {
        set({ selectedKeyframeIds: new Set([keyframeId]) });
      }
    },

    deselectAllKeyframes: () => {
      set({ selectedKeyframeIds: new Set() });
    },

    deleteSelectedKeyframes: () => {
      const { selectedKeyframeIds, clipKeyframes, invalidateCache } = get();
      if (selectedKeyframeIds.size === 0) return;

      const newMap = new Map<string, Keyframe[]>();

      clipKeyframes.forEach((keyframes, clipId) => {
        const filtered = keyframes.filter(k => !selectedKeyframeIds.has(k.id));
        if (filtered.length > 0) {
          newMap.set(clipId, filtered);
        }
      });

      set({
        clipKeyframes: newMap,
        selectedKeyframeIds: new Set(),
      });
      invalidateCache();
    },

    // === MASK MANAGEMENT ===
    setMaskEditMode: (mode) => {
      set({ maskEditMode: mode, maskDrawStart: null });
      if (mode === 'none') {
        set({ activeMaskId: null, selectedVertexIds: new Set() });
      }
    },

    setMaskDrawStart: (point) => {
      set({ maskDrawStart: point });
    },

    setActiveMask: (clipId, maskId) => {
      set({ activeMaskId: maskId, selectedVertexIds: new Set() });
      if (clipId && maskId) {
        set({ maskEditMode: 'editing' });
      }
    },

    selectVertex: (vertexId, addToSelection = false) => {
      const { selectedVertexIds } = get();
      if (addToSelection) {
        const newSet = new Set(selectedVertexIds);
        if (newSet.has(vertexId)) {
          newSet.delete(vertexId);
        } else {
          newSet.add(vertexId);
        }
        set({ selectedVertexIds: newSet });
      } else {
        set({ selectedVertexIds: new Set([vertexId]) });
      }
    },

    deselectAllVertices: () => {
      set({ selectedVertexIds: new Set() });
    },

    // Mask CRUD
    addMask: (clipId, maskData) => {
      const { clips, invalidateCache } = get();
      const maskId = `mask-${Date.now()}-${Math.random().toString(36).substr(2, 5)}`;

      const existingMasks = clips.find(c => c.id === clipId)?.masks || [];
      const maskCount = existingMasks.length + 1;

      const newMask: ClipMask = {
        id: maskId,
        name: maskData?.name || `Mask ${maskCount}`,
        vertices: maskData?.vertices || [],
        closed: maskData?.closed ?? false,
        opacity: maskData?.opacity ?? 1,
        feather: maskData?.feather ?? 0,
        featherQuality: maskData?.featherQuality ?? 50, // 1-100 (1-33=low, 34-66=medium, 67-100=high)
        inverted: maskData?.inverted ?? false,
        mode: maskData?.mode ?? 'add',
        expanded: maskData?.expanded ?? true,
        position: maskData?.position ?? { x: 0, y: 0 },
        visible: maskData?.visible ?? true,
      };

      set({
        clips: clips.map(c =>
          c.id === clipId
            ? { ...c, masks: [...(c.masks || []), newMask] }
            : c
        ),
      });

      invalidateCache();
      return maskId;
    },

    removeMask: (clipId, maskId) => {
      const { clips, activeMaskId, invalidateCache } = get();

      set({
        clips: clips.map(c =>
          c.id === clipId
            ? { ...c, masks: (c.masks || []).filter(m => m.id !== maskId) }
            : c
        ),
        activeMaskId: activeMaskId === maskId ? null : activeMaskId,
      });

      invalidateCache();
    },

    updateMask: (clipId, maskId, updates) => {
      const { clips, invalidateCache } = get();

      set({
        clips: clips.map(c =>
          c.id === clipId
            ? {
                ...c,
                masks: (c.masks || []).map(m =>
                  m.id === maskId ? { ...m, ...updates } : m
                ),
              }
            : c
        ),
      });

      invalidateCache();
    },

    reorderMasks: (clipId, fromIndex, toIndex) => {
      const { clips, invalidateCache } = get();
      const clip = clips.find(c => c.id === clipId);
      if (!clip?.masks) return;

      const masks = [...clip.masks];
      const [removed] = masks.splice(fromIndex, 1);
      masks.splice(toIndex, 0, removed);

      set({
        clips: clips.map(c =>
          c.id === clipId ? { ...c, masks } : c
        ),
      });

      invalidateCache();
    },

    getClipMasks: (clipId) => {
      const { clips } = get();
      return clips.find(c => c.id === clipId)?.masks || [];
    },

    // Vertex CRUD
    addVertex: (clipId, maskId, vertexData, index) => {
      const { clips, invalidateCache } = get();
      const vertexId = `vertex-${Date.now()}-${Math.random().toString(36).substr(2, 5)}`;

      const newVertex: MaskVertex = {
        id: vertexId,
        x: vertexData.x,
        y: vertexData.y,
        handleIn: vertexData.handleIn || { x: 0, y: 0 },
        handleOut: vertexData.handleOut || { x: 0, y: 0 },
      };

      set({
        clips: clips.map(c => {
          if (c.id !== clipId) return c;
          return {
            ...c,
            masks: (c.masks || []).map(m => {
              if (m.id !== maskId) return m;
              const vertices = [...m.vertices];
              if (index !== undefined) {
                vertices.splice(index, 0, newVertex);
              } else {
                vertices.push(newVertex);
              }
              return { ...m, vertices };
            }),
          };
        }),
      });

      invalidateCache();
      return vertexId;
    },

    removeVertex: (clipId, maskId, vertexId) => {
      const { clips, selectedVertexIds, invalidateCache } = get();

      set({
        clips: clips.map(c => {
          if (c.id !== clipId) return c;
          return {
            ...c,
            masks: (c.masks || []).map(m => {
              if (m.id !== maskId) return m;
              return {
                ...m,
                vertices: m.vertices.filter(v => v.id !== vertexId),
              };
            }),
          };
        }),
        selectedVertexIds: new Set(
          Array.from(selectedVertexIds).filter(id => id !== vertexId)
        ),
      });

      invalidateCache();
    },

    updateVertex: (clipId, maskId, vertexId, updates) => {
      const { clips, invalidateCache } = get();

      set({
        clips: clips.map(c => {
          if (c.id !== clipId) return c;
          return {
            ...c,
            masks: (c.masks || []).map(m => {
              if (m.id !== maskId) return m;
              return {
                ...m,
                vertices: m.vertices.map(v =>
                  v.id === vertexId ? { ...v, ...updates } : v
                ),
              };
            }),
          };
        }),
      });

      invalidateCache();
    },

    closeMask: (clipId, maskId) => {
      const { updateMask } = get();
      updateMask(clipId, maskId, { closed: true });
    },

    // Preset shapes
    addRectangleMask: (clipId) => {
      const { addMask, invalidateCache } = get();
      const maskId = addMask(clipId, { name: 'Rectangle Mask' });

      // Add rectangle vertices (normalized 0-1 coordinates)
      // Default rectangle covers 80% of the clip area, centered
      const margin = 0.1;
      const vertices: MaskVertex[] = [
        { id: `v-${Date.now()}-1`, x: margin, y: margin, handleIn: { x: 0, y: 0 }, handleOut: { x: 0, y: 0 } },
        { id: `v-${Date.now()}-2`, x: 1 - margin, y: margin, handleIn: { x: 0, y: 0 }, handleOut: { x: 0, y: 0 } },
        { id: `v-${Date.now()}-3`, x: 1 - margin, y: 1 - margin, handleIn: { x: 0, y: 0 }, handleOut: { x: 0, y: 0 } },
        { id: `v-${Date.now()}-4`, x: margin, y: 1 - margin, handleIn: { x: 0, y: 0 }, handleOut: { x: 0, y: 0 } },
      ];

      const { clips } = get();
      set({
        clips: clips.map(c => {
          if (c.id !== clipId) return c;
          return {
            ...c,
            masks: (c.masks || []).map(m =>
              m.id === maskId ? { ...m, vertices, closed: true } : m
            ),
          };
        }),
      });

      invalidateCache();
      return maskId;
    },

    addEllipseMask: (clipId) => {
      const { addMask, invalidateCache } = get();
      const maskId = addMask(clipId, { name: 'Ellipse Mask' });

      // Create ellipse using bezier curves (approximation)
      // Control point offset for circular bezier (~0.5523)
      const k = 0.5523;
      const cx = 0.5;
      const cy = 0.5;
      const rx = 0.4;
      const ry = 0.4;

      const vertices: MaskVertex[] = [
        // Top
        {
          id: `v-${Date.now()}-1`,
          x: cx,
          y: cy - ry,
          handleIn: { x: -rx * k, y: 0 },
          handleOut: { x: rx * k, y: 0 },
        },
        // Right
        {
          id: `v-${Date.now()}-2`,
          x: cx + rx,
          y: cy,
          handleIn: { x: 0, y: -ry * k },
          handleOut: { x: 0, y: ry * k },
        },
        // Bottom
        {
          id: `v-${Date.now()}-3`,
          x: cx,
          y: cy + ry,
          handleIn: { x: rx * k, y: 0 },
          handleOut: { x: -rx * k, y: 0 },
        },
        // Left
        {
          id: `v-${Date.now()}-4`,
          x: cx - rx,
          y: cy,
          handleIn: { x: 0, y: ry * k },
          handleOut: { x: 0, y: -ry * k },
        },
      ];

      const { clips } = get();
      set({
        clips: clips.map(c => {
          if (c.id !== clipId) return c;
          return {
            ...c,
            masks: (c.masks || []).map(m =>
              m.id === maskId ? { ...m, vertices, closed: true } : m
            ),
          };
        }),
      });

      invalidateCache();
      return maskId;
    },
  }))
);

// Helper function to get default effect parameters
function getDefaultEffectParams(type: string): Record<string, number | boolean | string> {
  switch (type) {
    case 'hue-shift':
      return { shift: 0 };
    case 'saturation':
      return { amount: 1 };
    case 'brightness':
      return { amount: 0 };
    case 'contrast':
      return { amount: 1 };
    case 'blur':
      return { radius: 0 };
    case 'pixelate':
      return { size: 8 };
    case 'kaleidoscope':
      return { segments: 6, rotation: 0 };
    case 'mirror':
      return { horizontal: true, vertical: false };
    case 'invert':
      return {};
    case 'rgb-split':
      return { amount: 0.01, angle: 0 };
    case 'levels':
      return { inputBlack: 0, inputWhite: 1, gamma: 1, outputBlack: 0, outputWhite: 1 };
    default:
      return {};
  }
}
